<!DOCTYPE html>
<html lang="en-US">
  
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Ziqi Liu</title>
    <meta name="description" content="Ziqi Liu. A PhD Candidate @ UT Austin.">
    <link rel="icon" href="logo.jpg">
    
  <link rel="preload" href="assets/css/0.styles.df94df16.css" as="style"><link rel="preload" href="assets/js/app.818c8079.js" as="script"><link rel="preload" href="assets/js/4.0e114e2d.js" as="script"><link rel="preload" href="assets/js/17.4353087f.js" as="script"><link rel="preload" href="assets/js/12.3117e98e.js" as="script"><link rel="preload" href="assets/js/11.b7d58d23.js" as="script"><link rel="preload" href="assets/js/6.025f0d73.js" as="script"><link rel="preload" href="assets/js/8.fde5ed7a.js" as="script"><link rel="preload" href="assets/js/10.ff4d8706.js" as="script"><link rel="prefetch" href="assets/js/13.3fe2f237.js"><link rel="prefetch" href="assets/js/14.f768ff21.js"><link rel="prefetch" href="assets/js/15.cb229c59.js"><link rel="prefetch" href="assets/js/16.ea53599e.js"><link rel="prefetch" href="assets/js/18.b209fa1c.js"><link rel="prefetch" href="assets/js/19.353daa00.js"><link rel="prefetch" href="assets/js/2.8b2c6820.js"><link rel="prefetch" href="assets/js/20.8f421ef9.js"><link rel="prefetch" href="assets/js/21.a7eb927c.js"><link rel="prefetch" href="assets/js/22.fa778b43.js"><link rel="prefetch" href="assets/js/23.60c399d5.js"><link rel="prefetch" href="assets/js/3.46e280fb.js"><link rel="prefetch" href="assets/js/5.b204be94.js"><link rel="prefetch" href="assets/js/7.35413e6c.js"><link rel="prefetch" href="assets/js/9.18b34838.js">

    <link rel="stylesheet" href="assets/css/0.styles.df94df16.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container no-sidebar home-page"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="index.html" class="home-link router-link-exact-active router-link-active"><!----> <span class="site-name">Yingwei Li</span></a> <div class="links"><!----> <nav class="nav-links can-hide"><div class="nav-item"><a href="index.html" class="nav-link router-link-exact-active router-link-active">
  Home
</a></div><div class="nav-item"><a href="publications/index.html" class="nav-link">
  Publications
</a></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="index.html" class="nav-link router-link-exact-active router-link-active">
  Home
</a></div><div class="nav-item"><a href="publications/index.html" class="nav-link">
  Publications
</a></div> <!----></nav>  <!----> </aside> <main class="page"> <div class="theme-default-content content__default"><div class="profile"><div class="image"><img src="profile.jpg" alt></div> <div class="info"><div class="name">
      Yingwei Li
    </div> <div class="bio"><p>A research scientist @ Waymo</p></div> <div class="socials"><div><a href="CV_Yingwei.pdf" target="_blank"><img src="icons/CV.svg" alt="Curriculum Vitae" title="Curriculum Vitae"></a></div><div><a href="https://scholar.google.com/citations?hl=en&amp;user=phWmJeIAAAAJ" target="_blank"><img src="icons/Google%20Scholar.svg" alt="Google Scholar" title="Google Scholar"></a></div><div><a href="https://github.com/LiYingwei" target="_blank"><img src="icons/github.svg" alt="GitHub" title="GitHub"></a></div><div><a href="https://twitter.com/yingwei_li?ref_src=twsrc%5Etfw" target="_blank"><img src="icons/twitter.png" alt="Twitter" title="Twitter"></a></div></div> <div class="contact"><div title="Contact me" class="email">Email: ywli@waymo.com</div></div> <!----></div></div> <h2 id="about-me">About Me</h2> <p>Hi! I am a research scientist at Waymo. I received my Ph.D. degree from <a href="https://www.cs.jhu.edu/" target="_blank" rel="noopener noreferrer">Computer Science<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> department at <a href="https://www.jhu.edu/" target="_blank" rel="noopener noreferrer">Johns Hopkins University<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>, advised by <a href="https://en.wikipedia.org/wiki/Bloomberg_Distinguished_Professorships" target="_blank" rel="noopener noreferrer">Bloomberg Distinguished Professor<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> <a href="https://www.cs.jhu.edu/~ayuille/" target="_blank" rel="noopener noreferrer">Dr. Alan Yuille<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>.</p> <p>I obtained B.S. in Computer Science at <a href="https://en.wikipedia.org/wiki/Fudan_University" target="_blank" rel="noopener noreferrer">Fudan University<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> in 2018. I also spent time at <a href="https://research.google/teams/cloud-ai/" target="_blank" rel="noopener noreferrer">Google Research<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>, <a href="https://waymo.com/" target="_blank" rel="noopener noreferrer">Waymo<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>, <a href="https://www.bytedance.com/en/" target="_blank" rel="noopener noreferrer">ByteDance<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>, <a href="https://www.ntu.edu.tw/english/" target="_blank" rel="noopener noreferrer">NTU<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>, and <a href="https://www.tusimple.com/" target="_blank" rel="noopener noreferrer">TuSimple<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>.</p> <p>My research interests mainly lie in computer vision, especially in autonomous driving, robust representation learning, multi-modality fusion, automated machine learning, and medical machine intelligence.</p> <affiliations frontmatter="[object Object]"></affiliations> <h2 id="news">News</h2> <div class="news"><ul><div><div><div><li><p><b><mark><font color=green>NEW</font></mark></b> [10/10/2022] One paper is accepted by <b>WACV 2023</b>.</p></li></div></div></div><div><div><div><li><p><b><mark><font color=green>NEW</font></mark></b> [09/17/2022] One paper is accepted by <b>ACM CCS 2022</b>.</p></li></div></div></div><div><div><div><li><p>[07/03/2022] One paper is accepted by <b>ECCV 2022</b>.</p></li></div></div></div><div><div><div><li><p>[06/06/2022] Begin my full-time work journey!</p></li></div></div></div><div><div><div><li><p>[05/05/2022] I finally passed my PhD thesis defense!</p></li></div></div></div><div><div><div><li><p>[04/12/2022] I will join Waymo as a research scientist.</p></li></div></div></div><div><div><div><li><p>[03/02/2022] Three papers are accepted by <b>CVPR 2022</b>.</p></li></div></div></div><div><div><div><li><p>[01/31/2022] One paper is accepted by <b>ICRA 2022</b>.</p></li></div></div></div><div><!----></div><div><!----></div> <div id="show_news_botton"><li><a>(view more)</a></li></div> <div id="hidden_news" onMouseout="hidden();" style="display: none"><div><!----></div><div><!----></div><div><!----></div><div><!----></div><div><!----></div><div><!----></div><div><!----></div><div><!----></div><div><div><li><p>[01/20/2022] <a href="https://openreview.net/pdf?id=MQ2sAGunyBP" target="_blank">R4D</a> is accepted by <b>ICLR 2022</b>.</p></li></div></div><div><div><li><p>[01/20/2022] <a href="https://openreview.net/pdf?id=hcoswsDHNAW" target="_blank">Fast AdvProp</a> is accepted by <b>ICLR 2022</b>.</p></li></div></div></div> <div id="hide_news_botton" style="display: none"><li><a>(view less)</a></li></div></ul></div> <h2 id="selected-publications">Selected Publications</h2> <div class="papers"><div><div><!----></div><div><!----></div><div><!----></div><div><div><div class="paper show-border"><div><div class="paper-image"><img src="publications/guo2022context.png" alt></div></div> <div class="paper-content"><p><b>Context Enhanced Stereo Transformer</b></p> <p>Weiyu Guo, Zhaoshuo Li, Yongkui Yang, Zheng Wang, Russ Taylor, Mathias Unberath, Alan Yuille, <b>Yingwei Li</b></p> <i><p>ECCV, 2022</p></i> <!----> <span>
              [<a href="https://arxiv.org/pdf/2210.11719.pdf" target="_blank">Paper</a>]
            </span> <span>
              [<a href="https://github.com/guoweiyu/Context-Enhanced-Stereo-Transformer" target="_blank">Code</a>]
            </span> <!----> <!----> <!----> <!----> <span>
              [<a shape="rect" class="toggle">Bibtex</a>]
            </span> <table bgcolor="#ace5f2" id="abs_Context Enhanced Stereo Transformer" style="display: none"><tr><td><span></span></td></tr></table> <table bgcolor="#ace5f2" id="Context Enhanced Stereo Transformer" style="display: none"><tr><td><div>@inproceedings{guo2022context,</br>title={Context Enhanced Stereo Transformer},</br>author={Guo, Weiyu and Li, Zhaoshuo and Yang, Yongkui and Wang, Zheng and Taylor, Russ and Unberath, Mathias and Yuille, Alan and Li, Yingwei},</br>booktitle={ECCV},</br>year={2022}</br>}</div></td></tr></table></div></div></div></div><div><div><div class="paper show-border"><div><div class="paper-image"><img src="publications/li2022lidarcamera.jpg" alt></div></div> <div class="paper-content"><p><b>DeepFusion: Lidar-Camera Deep Fusion for Multi-Modal 3D Object Detection</b></p> <p><b>Yingwei Li</b>, Adams Yu, Tianjian Meng, Ben Caine, Jiquan Ngiam, Daiyi Peng, Junyang Shen, Bo Wu, Yifeng Lu, Denny Zhou, Quoc Le, Alan Yuille, Mingxing Tan</p> <i><p>CVPR, 2022</p></i> <!----> <span>
              [<a href="https://arxiv.org/pdf/2203.08195.pdf" target="_blank">Paper</a>]
            </span> <span>
              [<a href="https://github.com/tensorflow/lingvo" target="_blank">Code</a>]
            </span> <span>
              [<a href="https://ai.googleblog.com/2022/04/lidar-camera-deep-fusion-for-multi.html" target="_blank">Website</a>]
            </span> <!----> <!----> <!----> <span>
              [<a shape="rect" class="toggle">Bibtex</a>]
            </span> <table bgcolor="#ace5f2" id="abs_DeepFusion: Lidar-Camera Deep Fusion for Multi-Modal 3D Object Detection" style="display: none"><tr><td><span></span></td></tr></table> <table bgcolor="#ace5f2" id="DeepFusion: Lidar-Camera Deep Fusion for Multi-Modal 3D Object Detection" style="display: none"><tr><td><div>@article{li2022deepfusion,</br>title={DeepFusion: Lidar-Camera Deep Fusion for Multi-Modal 3D Object Detection},</br>author={Li, Yingwei and Yu, Adams Wei and Meng, Tianjian and Caine, Ben and Ngiam, Jiquan and Peng, Daiyi and Shen, Junyang and Wu, Bo and Lu, Yifeng and Zhou, Denny and others},</br>journal={arXiv preprint arXiv:2203.08195},</br>year={2022}</br>}</div></td></tr></table></div></div></div></div><div><div><div class="paper show-border"><div><div class="paper-image"><img src="publications/xiao2022learning.jpg" alt></div></div> <div class="paper-content"><p><b>Learning from Temporal Gradient for Semi-supervised Action Recognition</b></p> <p>Junfei Xiao, Longlong Jing, Lin Zhang, Ju He, Qi She, Zongwei Zhou, Alan Yuille, <b>Yingwei Li</b></p> <i><p>CVPR, 2022</p></i> <!----> <span>
              [<a href="https://arxiv.org/pdf/2111.13241.pdf" target="_blank">Paper</a>]
            </span> <span>
              [<a href="https://github.com/lambert-x/video-semisup" target="_blank">Code</a>]
            </span> <!----> <!----> <!----> <!----> <span>
              [<a shape="rect" class="toggle">Bibtex</a>]
            </span> <table bgcolor="#ace5f2" id="abs_Learning from Temporal Gradient for Semi-supervised Action Recognition" style="display: none"><tr><td><span></span></td></tr></table> <table bgcolor="#ace5f2" id="Learning from Temporal Gradient for Semi-supervised Action Recognition" style="display: none"><tr><td><div>@article{xiao2021learning,</br>title={Learning from Temporal Gradient for Semi-supervised Action Recognition},</br>author={Xiao, Junfei and Jing, Longlong and Zhang, Lin and He, Ju and She, Qi and Zhou, Zongwei and Yuille, Alan and Li, Yingwei},</br>journal={arXiv preprint arXiv:2111.13241},</br>year={2021}</br>}</div></td></tr></table></div></div></div></div><div><!----></div><div><!----></div><div><div><div class="paper show-border"><div><div class="paper-image"><img src="publications/li2022r4d.jpg" alt></div></div> <div class="paper-content"><p><b>R4D: Utilizing Reference Objects for Long-Range Distance Estimation</b></p> <p><b>Yingwei Li</b>, Tiffany Chen, Maya Kabkab, Ruichi Yu, Longlong Jing, Yurong You, Hang Zhao</p> <i><p>ICLR, 2022</p></i> <!----> <span>
              [<a href="https://openreview.net/pdf?id=MQ2sAGunyBP" target="_blank">Paper</a>]
            </span> <!----> <!----> <!----> <!----> <span>
              [<a href="data/iclr22-r4d-supp.pdf" target="_blank">Supplementary</a>]
            </span> <span>
              [<a shape="rect" class="toggle">Bibtex</a>]
            </span> <table bgcolor="#ace5f2" id="abs_R4D: Utilizing Reference Objects for Long-Range Distance Estimation" style="display: none"><tr><td><span></span></td></tr></table> <table bgcolor="#ace5f2" id="R4D: Utilizing Reference Objects for Long-Range Distance Estimation" style="display: none"><tr><td><div>@inproceedings{li2021r4d,</br>title={R4D: Utilizing Reference Objects for Long-Range Distance Estimation},</br>author={Li, Yingwei and Chen, Tiffany and Kabkab, Maya and Yu, Ruichi and Jing, Longlong and You, Yurong and Zhao, Hang},</br>booktitle={International Conference on Learning Representations},</br>year={2021}</br>}</div></td></tr></table></div></div></div></div><div><!----></div><div><!----></div><div><div><div class="paper show-border"><div><div class="paper-image"><img src="publications/li2020shapetexture.jpg" alt></div></div> <div class="paper-content"><p><b>Shape-Texture Debiased Neural Network Training</b></p> <p><b>Yingwei Li</b>, Qihang Yu, Mingxing Tan, Jieru Mei, Peng Tang, Wei Shen, Alan Yuille, Cihang Xie</p> <i><p>ICLR, 2021</p></i> <!----> <span>
              [<a href="https://arxiv.org/pdf/2010.05981.pdf" target="_blank">Paper</a>]
            </span> <span>
              [<a href="https://github.com/LiYingwei/ShapeTextureDebiasedTraining" target="_blank">Code</a>]
            </span> <span>
              [<a href="https://iclr.cc/media/iclr-2021/Slides/2870.pdf" target="_blank">Website</a>]
            </span> <span>
              [<a href="https://slideslive.com/38953458/shapetexture-debiased-neural-network-training" target="_blank">Video</a>]
            </span> <!----> <!----> <span>
              [<a shape="rect" class="toggle">Bibtex</a>]
            </span> <table bgcolor="#ace5f2" id="abs_Shape-Texture Debiased Neural Network Training" style="display: none"><tr><td><span></span></td></tr></table> <table bgcolor="#ace5f2" id="Shape-Texture Debiased Neural Network Training" style="display: none"><tr><td><div>@article{li2020shape,</br>title={Shape-texture debiased neural network training},</br>author={Li, Yingwei and Yu, Qihang and Tan, Mingxing and Mei, Jieru and Tang, Peng and Shen, Wei and Yuille, Alan and Xie, Cihang},</br>journal={arXiv preprint arXiv:2010.05981},</br>year={2020}</br>}</div></td></tr></table></div></div></div></div><div><!----></div><div><!----></div><div><div><div class="paper show-border"><div><div class="paper-image"><img src="publications/li2019regional.jpg" alt></div></div> <div class="paper-content"><p><b>Regional Homogeneity: Towards Learning Transferable Universal Adversarial Perturbations Against Defenses</b></p> <p><b>Yingwei Li</b>, Song Bai, Cihang Xie, Zhenyu Liao, Xiaohui Shen, Alan Yuille</p> <i><p>ECCV, 2020</p></i> <!----> <span>
              [<a href="https://arxiv.org/pdf/1904.00979.pdf" target="_blank">Paper</a>]
            </span> <span>
              [<a href="https://github.com/LiYingwei/Regional-Homogeneity" target="_blank">Code</a>]
            </span> <!----> <!----> <!----> <!----> <span>
              [<a shape="rect" class="toggle">Bibtex</a>]
            </span> <table bgcolor="#ace5f2" id="abs_Regional Homogeneity: Towards Learning Transferable Universal Adversarial Perturbations Against Defenses" style="display: none"><tr><td><span></span></td></tr></table> <table bgcolor="#ace5f2" id="Regional Homogeneity: Towards Learning Transferable Universal Adversarial Perturbations Against Defenses" style="display: none"><tr><td><div>@inproceedings{li2020regional,</br>title={Regional homogeneity: Towards learning transferable universal adversarial perturbations against defenses},</br>author={Li, Yingwei and Bai, Song and Xie, Cihang and Liao, Zhenyu and Shen, Xiaohui and Yuille, Alan},</br>booktitle={European Conference on Computer Vision},</br>pages={795--813},</br>year={2020},</br>organization={Springer}</br>}</div></td></tr></table></div></div></div></div><div><div><div class="paper show-border"><div><div class="paper-image"><img src="publications/li2020neural.png" alt></div></div> <div class="paper-content"><p><b>Neural Architecture Search for Lightweight Non-Local Networks</b></p> <p><b>Yingwei Li</b>, Xiaojie Jin, Jieru Mei, Xiaochen Lian, Linjie Yang, Cihang Xie, Qihang Yu, Yuyin Zhou, Song Bai, Alan Yuille</p> <i><p>CVPR, 2020</p></i> <!----> <span>
              [<a href="https://arxiv.org/pdf/2004.01961.pdf" target="_blank">Paper</a>]
            </span> <span>
              [<a href="https://github.com/LiYingwei/AutoNL" target="_blank">Code</a>]
            </span> <!----> <!----> <!----> <!----> <span>
              [<a shape="rect" class="toggle">Bibtex</a>]
            </span> <table bgcolor="#ace5f2" id="abs_Neural Architecture Search for Lightweight Non-Local Networks" style="display: none"><tr><td><span></span></td></tr></table> <table bgcolor="#ace5f2" id="Neural Architecture Search for Lightweight Non-Local Networks" style="display: none"><tr><td><div>@inproceedings{li2020neural,</br>title={Neural architecture search for lightweight non-local networks},</br>author={Li, Yingwei and Jin, Xiaojie and Mei, Jieru and Lian, Xiaochen and Yang, Linjie and Xie, Cihang and Yu, Qihang and Zhou, Yuyin and Bai, Song and Yuille, Alan L},</br>booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},</br>pages={10297--10306},</br>year={2020}</br>}</div></td></tr></table></div></div></div></div><div><!----></div><div><div><div class="paper show-border"><div><div class="paper-image"><img src="publications/li2020learning.jpg" alt></div></div> <div class="paper-content"><p><b>Learning Transferable Adversarial Examples via Ghost Networks</b></p> <p><b>Yingwei Li</b>, Song Bai, Yuyin Zhou, Cihang Xie, Zhishuai Zhang, Alan Yuille</p> <i><p>AAAI, 2020</br>CVPR Workshop <b>(Oral)</b>, 2019</p></i> <!----> <span>
              [<a href="https://arxiv.org/pdf/1812.03413.pdf" target="_blank">Paper</a>]
            </span> <span>
              [<a href="https://github.com/LiYingwei/ghost-network" target="_blank">Code</a>]
            </span> <!----> <!----> <!----> <!----> <span>
              [<a shape="rect" class="toggle">Bibtex</a>]
            </span> <table bgcolor="#ace5f2" id="abs_Learning Transferable Adversarial Examples via Ghost Networks" style="display: none"><tr><td><span></span></td></tr></table> <table bgcolor="#ace5f2" id="Learning Transferable Adversarial Examples via Ghost Networks" style="display: none"><tr><td><div>@inproceedings{li2020learning,</br>title={Learning Transferable Adversarial Examples via Ghost Networks},</br>author={Li, Yingwei and Bai, Song and Zhou, Yuyin and Xie, Cihang and Zhang, Zhishuai and Yuille, Alan},</br>booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},</br>volume={34},</br>year={2020}</br>}</div></td></tr></table></div></div></div></div><div><div><div class="paper show-border"><div><div class="paper-image"><img src="publications/li2019volumetric.jpg" alt></div></div> <div class="paper-content"><p><b>Volumetric Medical Image Segmentation: A 3D Deep Coarse-to-Fine Framework and Its Adversarial Examples</b></p> <p><b>Yingwei Li</b>*, Zhuotun Zhu* Yuyin Zhou, Yingda Xia, Wei Shen, Elliot K Fishman, Alan Yuille</p> <i><p>Book Chapter: Deep Learning and CNN for Medical Image Computing , 2019</p></i> <!----> <span>
              [<a href="https://arxiv.org/pdf/2010.16074.pdf" target="_blank">Paper</a>]
            </span> <!----> <!----> <!----> <!----> <!----> <span>
              [<a shape="rect" class="toggle">Bibtex</a>]
            </span> <table bgcolor="#ace5f2" id="abs_Volumetric Medical Image Segmentation: A 3D Deep Coarse-to-Fine Framework and Its Adversarial Examples" style="display: none"><tr><td><span></span></td></tr></table> <table bgcolor="#ace5f2" id="Volumetric Medical Image Segmentation: A 3D Deep Coarse-to-Fine Framework and Its Adversarial Examples" style="display: none"><tr><td><div>@Inbook{Li2019,</br>author='Li, Yingwei and Zhu, Zhuotun and Zhou, Yuyin and Xia, Yingda and Shen, Wei and Fishman, Elliot K. and Yuille, Alan L.',</br>title='Volumetric Medical Image Segmentation: A 3D Deep Coarse-to-Fine Framework and Its Adversarial Examples',</br>bookTitle='Deep Learning and Convolutional Neural Networks for Medical Imaging and Clinical Informatics',</br>year='2019',</br>publisher='Springer International Publishing',</br>address='Cham',</br>pages='69--91',</br>isbn='978-3-030-13969-8',</br>doi='10.1007/978-3-030-13969-8_4',</br>url='https://doi.org/10.1007/978-3-030-13969-8_4'</br>}</div></td></tr></table></div></div></div></div><div><!----></div><div><!----></div></div></div> <br> <p><a href="publications/index.html"><b>→ Full list</b></a></p> <h2 id="affiliations">Affiliations</h2> <div class="edu_affiliations"><div><figure><a href="https://www.jhu.edu/" target="_blank"><img src="affiliations/jhu_logo.jpg" alt="Johns Hopkins University" title="Johns Hopkins University"></a> <figcaption>JHU<br>2018 – 2022</figcaption></figure></div><div><figure><a href="https://www.ntu.edu.tw/english/index.html" target="_blank"><img src="affiliations/ntu_logo.jpg" alt="National Taiwan University" title="National Taiwan University"></a> <figcaption>NTU<br>2017</figcaption></figure></div><div><figure><a href="https://www.fudan.edu.cn/" target="_blank"><img src="affiliations/FDU.png" alt="Fudan University" title="Fudan University"></a> <figcaption>FDU<br>2014 – 2018</figcaption></figure></div></div> <div class="intern_affiliations"><div><figure><a href="https://waymo.com/" target="_blank"><img src="affiliations/waymo_logo.jpg" alt="Waymo" title="Waymo"></a> <figcaption>Waymo<br>2020, 2022–Now</figcaption></figure></div><div><figure><a href="https://research.google/" target="_blank"><img src="affiliations/google_logo.jpg" alt="Google Research" title="Google Research"></a> <figcaption>Google Research<br>2021–2022</figcaption></figure></div><div><figure><a href="https://www.bytedance.com/en/" target="_blank"><img src="affiliations/bytedance_logo.jpg" alt="ByteDance" title="ByteDance"></a> <figcaption>ByteDance<br>2019</figcaption></figure></div><div><figure><a href="https://www.tusimple.com/" target="_blank"><img src="affiliations/tusimple_logo.jpg" alt="TuSimple" title="TuSimple"></a> <figcaption>TuSimple<br>2016</figcaption></figure></div></div> <h2 id="academic-service">Academic Service</h2> <p><strong>Co-organizer</strong></p> <ul><li><p><a href="https://eccv22-arow.github.io/" target="_blank" rel="noopener noreferrer">Adversarial Robustness in the Real World<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> @ ECCV 2022</p></li> <li><p><a href="https://artofrobust.github.io/" target="_blank" rel="noopener noreferrer">The Art of Robustness: Devil and Angel in Adversarial Machine Learning<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> @ CVPR 2022</p></li> <li><p><a href="https://practical-dl.github.io/" target="_blank" rel="noopener noreferrer">Practical Deep Learning in the Wild<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> @ AAAI 2022</p></li> <li><p><a href="https://iccv21-adv-workshop.github.io/" target="_blank" rel="noopener noreferrer">Adversarial Robustness in the Real World<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> @ ICCV 2021</p></li> <li><p><a href="https://advm-workshop-2021.github.io/" target="_blank" rel="noopener noreferrer">Adversarial Learning for Multimedia<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> @ ACMMM 2021</p></li> <li><p><a href="https://iccv21-adv-workshop.github.io/" target="_blank" rel="noopener noreferrer">Adversarial Robustness in the Real World<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a> @ ECCV 2020</p></li></ul> <p><strong>Reviewer</strong></p> <ul><li><p>Journal: IEEE TIP, IEEE TDSC, Neurocomputing, Pattern Recognition.</p></li> <li><p>Conference: AmlCV@CVPR2020, SRML@ICML2021, SecMl@ICLR2021, RseMl@AAAI2021 AAAI 2021, IJCAI 2021, CVPR 2021, ICCV 2021, NeurIPS 2021, AAAI 2022, ICLR 2022, CVPR 2022, ICML 2022.</p></li></ul></div> <footer class="page-edit"><!----> <div class="last-updated"><span class="prefix">Last Updated:</span> <span class="time">11/13/2022, 3:41:22 PM</span></div></footer> <!----> </main> <div class="footer">
    
    
  </body>

</html>
