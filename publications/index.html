<!DOCTYPE html>
<html lang="en-US">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width,initial-scale=1">
  <title>All Projects | Ziqi Liu</title>
  <meta name="description" content="Ziqi Liu. A PhD Candidate @ UT Austin.">
  <link rel="icon" href="../logo.jpg">

  <link rel="preload" href="../assets/css/0.styles.df94df16.css" as="style">
  <link rel="stylesheet" href="../assets/css/0.styles.df94df16.css">
</head>

<body>
  <div id="app" data-server-rendered="true">
    <div class="theme-container no-sidebar">
      <header class="navbar">
        <div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img"
            viewBox="0 0 448 512" class="icon">
            <path fill="currentColor"
              d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z">
            </path>
          </svg></div> <a href="../index.html" class="home-link router-link-active"><!----> <span class="site-name">Ziqi
            Liu</span></a>
        <div class="links"><!---->
          <nav class="nav-links can-hide">
            <div class="nav-item"><a href="../index.html" class="nav-link">
                Home
              </a></div>
            <div class="nav-item"><a href="index.html" class="nav-link router-link-exact-active router-link-active">
                Projects
              </a></div> <!---->
          </nav>
        </div>
      </header>
      <div class="sidebar-mask"></div>
      <aside class="sidebar">
        <nav class="nav-links">
          <div class="nav-item"><a href="../index.html" class="nav-link">
              Home
            </a></div>
          <div class="nav-item"><a href="index.html" class="nav-link router-link-exact-active router-link-active">
              Projects
            </a></div> <!---->
        </nav> <!---->
      </aside>
      <main class="page">
        <div class="theme-default-content content__default">
          <div class="tagpapers">
            <h2>All Projects</h2>
            <div class="tags"><span class="tagpaper"><span><a href="index.html" target="_self"><u>All by
                      Year</u></a></span></span><span class="tagpaper"><span><a href="TransportationStudies.html"
                    target="_self">Transportation Studies</a></span></span><span class="tagpaper"><span><a
                    href="DesignWorks.html" target="_self">DesignWorks</a></span></span></div> <br>
            <div class="note">
              <font size="3" color="rgb(0,0,0)" style="font-weight:normal;">Representative papers are shown with red
                border</font>
            </div>
          </div>
          <div class="papers">
            <div>
              <div>
                <h1>
                  <font size="6">
                    <p>2023</p>
                  </font>
                </h1>
                <div><!----></div>
                <div>
                  <div>
                    <div class="paper show-border">
                      <div>
                        <div class="paper-image"><img src="chen2023class.jpg" alt></div>
                      </div>
                      <div class="paper-content">
                        <p><b>Class-Level Confidence Based 3D Semi-Supervised Learning</b></p>
                        <p>Zhimin Chen, Longlong Jing, Liang Yang, <b>Yingwei Li</b>, and Bing Li</p> <i>
                          <p>WACV, 2023</p>
                        </i> <!----> <span>
                          [<a href="https://arxiv.org/pdf/2210.10138.pdf" target="_blank">Paper</a>]
                        </span> <!----> <!----> <!----> <!----> <!----> <span>
                          [<a shape="rect" class="toggle">Bibtex</a>]
                        </span>
                        <table bgcolor="#ace5f2" id="abs_Class-Level Confidence Based 3D Semi-Supervised Learning"
                          style="display: none">
                          <tr>
                            <td><span></span></td>
                          </tr>
                        </table>
                        <table bgcolor="#ace5f2" id="Class-Level Confidence Based 3D Semi-Supervised Learning"
                          style="display: none">
                          <tr>
                            <td>
                              <div>@article{chen2022class,</br> title={Class-Level Confidence Based 3D Semi-Supervised
                                Learning},</br> author={Chen, Zhimin and Jing, Longlong and Yang, Liang and Li,
                                Bing},</br> journal={arXiv preprint arXiv:2210.10138},</br> year={2022}</br>}</div>
                            </td>
                          </tr>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
                <div><!----></div>
                <div><!----></div>
                <div><!----></div>
                <div><!----></div>
                <div><!----></div>
                <div><!----></div>
                <div><!----></div>
                <div><!----></div>
                <div><!----></div>
                <div><!----></div>
                <div><!----></div>
                <div><!----></div>
                <div><!----></div>
                <div><!----></div>
                <div><!----></div>
                <div><!----></div>
                <div><!----></div>
                <div><!----></div>
                <div><!----></div>
              </div>
              <div>
                <h1>
                  <font size="6">
                    <p>2022</p>
                  </font>
                </h1>
                <div><!----></div>
                <div><!----></div>
                <div>
                  <div>
                    <div class="paper show-border">
                      <div>
                        <div class="paper-image"><img src="liu2022harnessing.jpg" alt></div>
                      </div>
                      <div class="paper-content">
                        <p><b>Harnessing Perceptual Adversarial Patches for Crowd Counting</b></p>
                        <p>Shunchang Liu*, Jiakai Wang*, Aishan Liu, <b>Yingwei Li</b>, Yijie Gao, Xianglong Liu,
                          Dacheng Tao</p> <i>
                          <p>ACM CCS, 2022</p>
                        </i> <!----> <span>
                          [<a href="https://dl.acm.org/doi/pdf/10.1145/3548606.3560566" target="_blank">Paper</a>]
                        </span> <span>
                          [<a href="https://github.com/shunchang-liu/PAP-Pytorch" target="_blank">Code</a>]
                        </span> <!----> <!----> <!----> <!----> <span>
                          [<a shape="rect" class="toggle">Bibtex</a>]
                        </span>
                        <table bgcolor="#ace5f2" id="abs_Harnessing Perceptual Adversarial Patches for Crowd Counting"
                          style="display: none">
                          <tr>
                            <td><span></span></td>
                          </tr>
                        </table>
                        <table bgcolor="#ace5f2" id="Harnessing Perceptual Adversarial Patches for Crowd Counting"
                          style="display: none">
                          <tr>
                            <td>
                              <div>@inproceedings{liu2022harnessing,</br> title={Harnessing Perceptual Adversarial
                                Patches for Crowd Counting},</br> author={Liu, Shunchang and Wang, Jiakai and Liu,
                                Aishan and Li, Yingwei and Gao, Yijie and Liu, Xianglong and Tao, Dacheng},</br>
                                booktitle={Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications
                                Security},</br> pages={2055--2069},</br> year={2022}</br>}</div>
                            </td>
                          </tr>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
                <div>
                  <div>
                    <div class="paper show-border">
                      <div>
                        <div class="paper-image"><img src="guo2022context.png" alt></div>
                      </div>
                      <div class="paper-content">
                        <p><b>Context Enhanced Stereo Transformer</b></p>
                        <p>Weiyu Guo, Zhaoshuo Li, Yongkui Yang, Zheng Wang, Russ Taylor, Mathias Unberath, Alan Yuille,
                          <b>Yingwei Li</b></p> <i>
                          <p>ECCV, 2022</p>
                        </i> <!----> <span>
                          [<a href="https://arxiv.org/pdf/2210.11719.pdf" target="_blank">Paper</a>]
                        </span> <span>
                          [<a href="https://github.com/guoweiyu/Context-Enhanced-Stereo-Transformer"
                            target="_blank">Code</a>]
                        </span> <!----> <!----> <!----> <!----> <span>
                          [<a shape="rect" class="toggle">Bibtex</a>]
                        </span>
                        <table bgcolor="#ace5f2" id="abs_Context Enhanced Stereo Transformer" style="display: none">
                          <tr>
                            <td><span></span></td>
                          </tr>
                        </table>
                        <table bgcolor="#ace5f2" id="Context Enhanced Stereo Transformer" style="display: none">
                          <tr>
                            <td>
                              <div>@inproceedings{guo2022context,</br>title={Context Enhanced Stereo
                                Transformer},</br>author={Guo, Weiyu and Li, Zhaoshuo and Yang, Yongkui and Wang, Zheng
                                and Taylor, Russ and Unberath, Mathias and Yuille, Alan and Li,
                                Yingwei},</br>booktitle={ECCV},</br>year={2022}</br>}</div>
                            </td>
                          </tr>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
                <div>
                  <div>
                    <div class="paper show-border highlight">
                      <div>
                        <div class="paper-image"><img src="li2022lidarcamera.jpg" alt></div>
                      </div>
                      <div class="paper-content">
                        <p><b>DeepFusion: Lidar-Camera Deep Fusion for Multi-Modal 3D Object Detection</b></p>
                        <p><b>Yingwei Li</b>, Adams Yu, Tianjian Meng, Ben Caine, Jiquan Ngiam, Daiyi Peng, Junyang
                          Shen, Bo Wu, Yifeng Lu, Denny Zhou, Quoc Le, Alan Yuille, Mingxing Tan</p> <i>
                          <p>CVPR, 2022</p>
                        </i> <!----> <span>
                          [<a href="https://arxiv.org/pdf/2203.08195.pdf" target="_blank">Paper</a>]
                        </span> <span>
                          [<a href="https://github.com/tensorflow/lingvo" target="_blank">Code</a>]
                        </span> <span>
                          [<a href="https://ai.googleblog.com/2022/04/lidar-camera-deep-fusion-for-multi.html"
                            target="_blank">Website</a>]
                        </span> <!----> <!----> <!----> <span>
                          [<a shape="rect" class="toggle">Bibtex</a>]
                        </span>
                        <table bgcolor="#ace5f2"
                          id="abs_DeepFusion: Lidar-Camera Deep Fusion for Multi-Modal 3D Object Detection"
                          style="display: none">
                          <tr>
                            <td><span></span></td>
                          </tr>
                        </table>
                        <table bgcolor="#ace5f2"
                          id="DeepFusion: Lidar-Camera Deep Fusion for Multi-Modal 3D Object Detection"
                          style="display: none">
                          <tr>
                            <td>
                              <div>@article{li2022deepfusion,</br>title={DeepFusion: Lidar-Camera Deep Fusion for
                                Multi-Modal 3D Object Detection},</br>author={Li, Yingwei and Yu, Adams Wei and Meng,
                                Tianjian and Caine, Ben and Ngiam, Jiquan and Peng, Daiyi and Shen, Junyang and Wu, Bo
                                and Lu, Yifeng and Zhou, Denny and others},</br>journal={arXiv preprint
                                arXiv:2203.08195},</br>year={2022}</br>}</div>
                            </td>
                          </tr>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
                <div>
                  <div>
                    <div class="paper show-border">
                      <div>
                        <div class="paper-image"><img src="xiao2022learning.jpg" alt></div>
                      </div>
                      <div class="paper-content">
                        <p><b>Learning from Temporal Gradient for Semi-supervised Action Recognition</b></p>
                        <p>Junfei Xiao, Longlong Jing, Lin Zhang, Ju He, Qi She, Zongwei Zhou, Alan Yuille, <b>Yingwei
                            Li</b></p> <i>
                          <p>CVPR, 2022</p>
                        </i> <!----> <span>
                          [<a href="https://arxiv.org/pdf/2111.13241.pdf" target="_blank">Paper</a>]
                        </span> <span>
                          [<a href="https://github.com/lambert-x/video-semisup" target="_blank">Code</a>]
                        </span> <!----> <!----> <!----> <!----> <span>
                          [<a shape="rect" class="toggle">Bibtex</a>]
                        </span>
                        <table bgcolor="#ace5f2"
                          id="abs_Learning from Temporal Gradient for Semi-supervised Action Recognition"
                          style="display: none">
                          <tr>
                            <td><span></span></td>
                          </tr>
                        </table>
                        <table bgcolor="#ace5f2"
                          id="Learning from Temporal Gradient for Semi-supervised Action Recognition"
                          style="display: none">
                          <tr>
                            <td>
                              <div>@article{xiao2021learning,</br>title={Learning from Temporal Gradient for
                                Semi-supervised Action Recognition},</br>author={Xiao, Junfei and Jing, Longlong and
                                Zhang, Lin and He, Ju and She, Qi and Zhou, Zongwei and Yuille, Alan and Li,
                                Yingwei},</br>journal={arXiv preprint arXiv:2111.13241},</br>year={2021}</br>}</div>
                            </td>
                          </tr>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
                <div>
                  <div>
                    <div class="paper show-border">
                      <div>
                        <div class="paper-image"><img src="gupta2022swapmix.png" alt></div>
                      </div>
                      <div class="paper-content">
                        <p><b>SwapMix: Diagnosing and Regularizing the Over-reliance on Visual Context in Visual
                            Question Answering</b></p>
                        <p>Vipul Gupta, Zhuowan Li, Adam Kortylewski, Chenyu Zhang, <b>Yingwei Li</b>, Alan Yuille</p>
                        <i>
                          <p>CVPR, 2022</p>
                        </i> <!----> <span>
                          [<a href="https://arxiv.org/pdf/2204.02285.pdf" target="_blank">Paper</a>]
                        </span> <!----> <!----> <!----> <!----> <!----> <span>
                          [<a shape="rect" class="toggle">Bibtex</a>]
                        </span>
                        <table bgcolor="#ace5f2"
                          id="abs_SwapMix: Diagnosing and Regularizing the Over-reliance on Visual Context in Visual Question Answering"
                          style="display: none">
                          <tr>
                            <td><span></span></td>
                          </tr>
                        </table>
                        <table bgcolor="#ace5f2"
                          id="SwapMix: Diagnosing and Regularizing the Over-reliance on Visual Context in Visual Question Answering"
                          style="display: none">
                          <tr>
                            <td>
                              <div>@article{gupta2022swapmix,</br>title={SwapMix: Diagnosing and Regularizing the
                                Over-Reliance on Visual Context in Visual Question Answering},</br>author={Gupta, Vipul
                                and Li, Zhuowan and Kortylewski, Adam and Zhang, Chenyu and Li, Yingwei and Yuille,
                                Alan},</br>journal={arXiv preprint arXiv:2204.02285},</br>year={2022}</br>}</div>
                            </td>
                          </tr>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
                <div>
                  <div>
                    <div class="paper show-border">
                      <div>
                        <div class="paper-image"><img src="jing2022depth.jpg" alt></div>
                      </div>
                      <div class="paper-content">
                        <p><b>Depth Estimation Matters Most: Improving Per-Object Depth Estimation for Monocular 3D
                            Detection and Tracking</b></p>
                        <p>Longlong Jing, Ruichi Yu, Jiyang Gao, Henrik Kretzschmar, Kang Li, Charles R. Qi, Hang Zhao,
                          Alper Ayvaci, Xu Chen, Dillon Cower, <b>Yingwei Li</b>, Yurong You, Han Deng, Congcong Li,
                          Dragomir Anguelov</p> <i>
                          <p>ICRA, 2022</p>
                        </i> <!----> <!----> <!----> <!----> <!----> <!----> <!----> <!---->
                        <table bgcolor="#ace5f2"
                          id="abs_Depth Estimation Matters Most: Improving Per-Object Depth Estimation for Monocular 3D Detection and Tracking"
                          style="display: none">
                          <tr>
                            <td><span></span></td>
                          </tr>
                        </table>
                        <table bgcolor="#ace5f2"
                          id="Depth Estimation Matters Most: Improving Per-Object Depth Estimation for Monocular 3D Detection and Tracking"
                          style="display: none">
                          <tr>
                            <td>
                              <div></div>
                            </td>
                          </tr>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
                <div>
                  <div>
                    <div class="paper show-border highlight">
                      <div>
                        <div class="paper-image"><img src="li2022r4d.jpg" alt></div>
                      </div>
                      <div class="paper-content">
                        <p><b>R4D: Utilizing Reference Objects for Long-Range Distance Estimation</b></p>
                        <p><b>Yingwei Li</b>, Tiffany Chen, Maya Kabkab, Ruichi Yu, Longlong Jing, Yurong You, Hang Zhao
                        </p> <i>
                          <p>ICLR, 2022</p>
                        </i> <!----> <span>
                          [<a href="https://openreview.net/pdf?id=MQ2sAGunyBP" target="_blank">Paper</a>]
                        </span> <!----> <!----> <!----> <!----> <span>
                          [<a href="../data/iclr22-r4d-supp.pdf" target="_blank">Supplementary</a>]
                        </span> <span>
                          [<a shape="rect" class="toggle">Bibtex</a>]
                        </span>
                        <table bgcolor="#ace5f2"
                          id="abs_R4D: Utilizing Reference Objects for Long-Range Distance Estimation"
                          style="display: none">
                          <tr>
                            <td><span></span></td>
                          </tr>
                        </table>
                        <table bgcolor="#ace5f2"
                          id="R4D: Utilizing Reference Objects for Long-Range Distance Estimation"
                          style="display: none">
                          <tr>
                            <td>
                              <div>@inproceedings{li2021r4d,</br>title={R4D: Utilizing Reference Objects for Long-Range
                                Distance Estimation},</br>author={Li, Yingwei and Chen, Tiffany and Kabkab, Maya and Yu,
                                Ruichi and Jing, Longlong and You, Yurong and Zhao, Hang},</br>booktitle={International
                                Conference on Learning Representations},</br>year={2021}</br>}</div>
                            </td>
                          </tr>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
                <div>
                  <div>
                    <div class="paper show-border">
                      <div>
                        <div class="paper-image"><img src="mei2022fast.jpg" alt></div>
                      </div>
                      <div class="paper-content">
                        <p><b>Fast AdvProp</b></p>
                        <p>Jieru Mei, Yucheng Han, Yutong Bai, Yixiao Zhang, <b>Yingwei Li</b>, Xianhang Li, Alan
                          Yuille, Cihang Xie</p> <i>
                          <p>ICLR, 2022</p>
                        </i> <!----> <span>
                          [<a href="https://openreview.net/pdf?id=hcoswsDHNAW" target="_blank">Paper</a>]
                        </span> <!----> <!----> <!----> <!----> <!----> <span>
                          [<a shape="rect" class="toggle">Bibtex</a>]
                        </span>
                        <table bgcolor="#ace5f2" id="abs_Fast AdvProp" style="display: none">
                          <tr>
                            <td><span></span></td>
                          </tr>
                        </table>
                        <table bgcolor="#ace5f2" id="Fast AdvProp" style="display: none">
                          <tr>
                            <td>
                              <div>@inproceedings{mei2021fast,</br>title={Fast AdvProp},</br>author={Mei, Jieru and Han,
                                Yucheng and Bai, Yutong and Zhang, Yixiao and Li, Yingwei and Li, Xianhang and Yuille,
                                Alan and Xie, Cihang},</br>booktitle={International Conference on Learning
                                Representations},</br>year={2021}</br>}</div>
                            </td>
                          </tr>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
                <div><!----></div>
                <div><!----></div>
                <div><!----></div>
                <div><!----></div>
                <div><!----></div>
                <div><!----></div>
                <div><!----></div>
                <div><!----></div>
                <div><!----></div>
                <div><!----></div>
                <div><!----></div>
              </div>
              <div>
                <h1>
                  <font size="6">
                    <p>2021</p>
                  </font>
                </h1>
                <div><!----></div>
                <div><!----></div>
                <div><!----></div>
                <div><!----></div>
                <div><!----></div>
                <div><!----></div>
                <div><!----></div>
                <div><!----></div>
                <div><!----></div>
                <div><!----></div>
                <div>
                  <div>
                    <div class="paper show-border">
                      <div>
                        <div class="paper-image"><img src="pi2021searching.jpg" alt></div>
                      </div>
                      <div class="paper-content">
                        <p><b>Searching for TrioNet: Combining Convolution with Local and Global Self-Attention</b></p>
                        <p>Huaijin Pi, Huiyu Wang, <b>Yingwei Li</b>, Zizhang Li, Alan Yuille</p> <i>
                          <p>BMVC, 2021</p>
                        </i> <!----> <span>
                          [<a href="https://www.bmvc2021-virtualconference.com/assets/papers/0345.pdf"
                            target="_blank">Paper</a>]
                        </span> <span>
                          [<a href="https://github.com/phj128/TrioNet" target="_blank">Code</a>]
                        </span> <!----> <!----> <!----> <!----> <span>
                          [<a shape="rect" class="toggle">Bibtex</a>]
                        </span>
                        <table bgcolor="#ace5f2"
                          id="abs_Searching for TrioNet: Combining Convolution with Local and Global Self-Attention"
                          style="display: none">
                          <tr>
                            <td><span></span></td>
                          </tr>
                        </table>
                        <table bgcolor="#ace5f2"
                          id="Searching for TrioNet: Combining Convolution with Local and Global Self-Attention"
                          style="display: none">
                          <tr>
                            <td>
                              <div>@article{pi2021searching,</br>title={Searching for TrioNet: Combining Convolution
                                with Local and Global Self-Attention},</br>author={Pi, Huaijin and Wang, Huiyu and Li,
                                Yingwei and Li, Zizhang and Yuille, Alan},</br>journal={arXiv preprint
                                arXiv:2111.07547},</br>year={2021}</br>}</div>
                            </td>
                          </tr>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
                <div>
                  <div>
                    <div class="paper show-border highlight">
                      <div>
                        <div class="paper-image"><img src="li2020shapetexture.jpg" alt></div>
                      </div>
                      <div class="paper-content">
                        <p><b>Shape-Texture Debiased Neural Network Training</b></p>
                        <p><b>Yingwei Li</b>, Qihang Yu, Mingxing Tan, Jieru Mei, Peng Tang, Wei Shen, Alan Yuille,
                          Cihang Xie</p> <i>
                          <p>ICLR, 2021</p>
                        </i> <!----> <span>
                          [<a href="https://arxiv.org/pdf/2010.05981.pdf" target="_blank">Paper</a>]
                        </span> <span>
                          [<a href="https://github.com/LiYingwei/ShapeTextureDebiasedTraining" target="_blank">Code</a>]
                        </span> <span>
                          [<a href="https://iclr.cc/media/iclr-2021/Slides/2870.pdf" target="_blank">Website</a>]
                        </span> <span>
                          [<a href="https://slideslive.com/38953458/shapetexture-debiased-neural-network-training"
                            target="_blank">Video</a>]
                        </span> <!----> <!----> <span>
                          [<a shape="rect" class="toggle">Bibtex</a>]
                        </span>
                        <table bgcolor="#ace5f2" id="abs_Shape-Texture Debiased Neural Network Training"
                          style="display: none">
                          <tr>
                            <td><span></span></td>
                          </tr>
                        </table>
                        <table bgcolor="#ace5f2" id="Shape-Texture Debiased Neural Network Training"
                          style="display: none">
                          <tr>
                            <td>
                              <div>@article{li2020shape,</br>title={Shape-texture debiased neural network
                                training},</br>author={Li, Yingwei and Yu, Qihang and Tan, Mingxing and Mei, Jieru and
                                Tang, Peng and Shen, Wei and Yuille, Alan and Xie, Cihang},</br>journal={arXiv preprint
                                arXiv:2010.05981},</br>year={2020}</br>}</div>
                            </td>
                          </tr>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
                <div>
                  <div>
                    <div class="paper show-border">
                      <div>
                        <div class="paper-image"><img src="yu2020cakes.png" alt></div>
                      </div>
                      <div class="paper-content">
                        <p><b>CAKES: Channel-wise Automatic KErnel Shrinking for Efficient 3D Network</b></p>
                        <p>Qihang Yu, <b>Yingwei Li</b>, Jieru Mei, Yuyin Zhou, Alan Yuille</p> <i>
                          <p>AAAI, 2021</p>
                        </i> <!----> <span>
                          [<a href="https://arxiv.org/pdf/2003.12798.pdf" target="_blank">Paper</a>]
                        </span> <span>
                          [<a href="https://github.com/yucornetto/CAKES" target="_blank">Code</a>]
                        </span> <!----> <!----> <!----> <!----> <span>
                          [<a shape="rect" class="toggle">Bibtex</a>]
                        </span>
                        <table bgcolor="#ace5f2"
                          id="abs_CAKES: Channel-wise Automatic KErnel Shrinking for Efficient 3D Network"
                          style="display: none">
                          <tr>
                            <td><span></span></td>
                          </tr>
                        </table>
                        <table bgcolor="#ace5f2"
                          id="CAKES: Channel-wise Automatic KErnel Shrinking for Efficient 3D Network"
                          style="display: none">
                          <tr>
                            <td>
                              <div>@article{yu2020cakes,</br>title={Cakes: Channel-wise automatic kernel shrinking for
                                efficient 3d network},</br>author={Yu, Qihang and Li, Yingwei and Mei, Jieru and Zhou,
                                Yuyin and Yuille, Alan L},</br>journal={arXiv preprint
                                arXiv:2003.12798},</br>volume={2},</br>year={2020}</br>}</div>
                            </td>
                          </tr>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
                <div><!----></div>
                <div><!----></div>
                <div><!----></div>
                <div><!----></div>
                <div><!----></div>
                <div><!----></div>
                <div><!----></div>
                <div><!----></div>
              </div>
              <div>
                <h1>
                  <font size="6">
                    <p>2020</p>
                  </font>
                </h1>
                <div><!----></div>
                <div><!----></div>
                <div><!----></div>
                <div><!----></div>
                <div><!----></div>
                <div><!----></div>
                <div><!----></div>
                <div><!----></div>
                <div><!----></div>
                <div><!----></div>
                <div><!----></div>
                <div><!----></div>
                <div><!----></div>
                <div>
                  <div>
                    <div class="paper show-border">
                      <div>
                        <div class="paper-image"><img src="bai2019adversarial.png" alt></div>
                      </div>
                      <div class="paper-content">
                        <p><b>Adversarial Metric Attack and Defense for Person Re-identification</b></p>
                        <p>Song Bai, <b>Yingwei Li</b>, Yuyin Zhou, Qizhu Li, Philip H.S. Torr</p> <i>
                          <p>TPAMI, 2020</p>
                        </i> <!----> <span>
                          [<a href="https://arxiv.org/pdf/1901.10650.pdf" target="_blank">Paper</a>]
                        </span> <span>
                          [<a href="https://github.com/SongBaiHust/Adversarial_Metric_Attack" target="_blank">Code</a>]
                        </span> <!----> <!----> <!----> <!----> <span>
                          [<a shape="rect" class="toggle">Bibtex</a>]
                        </span>
                        <table bgcolor="#ace5f2"
                          id="abs_Adversarial Metric Attack and Defense for Person Re-identification"
                          style="display: none">
                          <tr>
                            <td><span></span></td>
                          </tr>
                        </table>
                        <table bgcolor="#ace5f2" id="Adversarial Metric Attack and Defense for Person Re-identification"
                          style="display: none">
                          <tr>
                            <td>
                              <div>@article{bai2020adversarial,</br>title={Adversarial metric attack and defense for
                                person re-identification},</br>author={Bai, Song and Li, Yingwei and Zhou, Yuyin and Li,
                                Qizhu and Torr, Philip HS},</br>journal={IEEE Transactions on Pattern Analysis and
                                Machine
                                Intelligence},</br>volume={43},</br>number={6},</br>pages={2119--2126},</br>year={2020},</br>publisher={IEEE}</br>}
                              </div>
                            </td>
                          </tr>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
                <div>
                  <div>
                    <div class="paper show-border">
                      <div>
                        <div class="paper-image"><img src="li2019regional.jpg" alt></div>
                      </div>
                      <div class="paper-content">
                        <p><b>Regional Homogeneity: Towards Learning Transferable Universal Adversarial Perturbations
                            Against Defenses</b></p>
                        <p><b>Yingwei Li</b>, Song Bai, Cihang Xie, Zhenyu Liao, Xiaohui Shen, Alan Yuille</p> <i>
                          <p>ECCV, 2020</p>
                        </i> <!----> <span>
                          [<a href="https://arxiv.org/pdf/1904.00979.pdf" target="_blank">Paper</a>]
                        </span> <span>
                          [<a href="https://github.com/LiYingwei/Regional-Homogeneity" target="_blank">Code</a>]
                        </span> <!----> <!----> <!----> <!----> <span>
                          [<a shape="rect" class="toggle">Bibtex</a>]
                        </span>
                        <table bgcolor="#ace5f2"
                          id="abs_Regional Homogeneity: Towards Learning Transferable Universal Adversarial Perturbations Against Defenses"
                          style="display: none">
                          <tr>
                            <td><span></span></td>
                          </tr>
                        </table>
                        <table bgcolor="#ace5f2"
                          id="Regional Homogeneity: Towards Learning Transferable Universal Adversarial Perturbations Against Defenses"
                          style="display: none">
                          <tr>
                            <td>
                              <div>@inproceedings{li2020regional,</br>title={Regional homogeneity: Towards learning
                                transferable universal adversarial perturbations against defenses},</br>author={Li,
                                Yingwei and Bai, Song and Xie, Cihang and Liao, Zhenyu and Shen, Xiaohui and Yuille,
                                Alan},</br>booktitle={European Conference on Computer
                                Vision},</br>pages={795--813},</br>year={2020},</br>organization={Springer}</br>}</div>
                            </td>
                          </tr>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
                <div>
                  <div>
                    <div class="paper show-border highlight">
                      <div>
                        <div class="paper-image"><img src="li2020neural.png" alt></div>
                      </div>
                      <div class="paper-content">
                        <p><b>Neural Architecture Search for Lightweight Non-Local Networks</b></p>
                        <p><b>Yingwei Li</b>, Xiaojie Jin, Jieru Mei, Xiaochen Lian, Linjie Yang, Cihang Xie, Qihang Yu,
                          Yuyin Zhou, Song Bai, Alan Yuille</p> <i>
                          <p>CVPR, 2020</p>
                        </i> <!----> <span>
                          [<a href="https://arxiv.org/pdf/2004.01961.pdf" target="_blank">Paper</a>]
                        </span> <span>
                          [<a href="https://github.com/LiYingwei/AutoNL" target="_blank">Code</a>]
                        </span> <!----> <!----> <!----> <!----> <span>
                          [<a shape="rect" class="toggle">Bibtex</a>]
                        </span>
                        <table bgcolor="#ace5f2" id="abs_Neural Architecture Search for Lightweight Non-Local Networks"
                          style="display: none">
                          <tr>
                            <td><span></span></td>
                          </tr>
                        </table>
                        <table bgcolor="#ace5f2" id="Neural Architecture Search for Lightweight Non-Local Networks"
                          style="display: none">
                          <tr>
                            <td>
                              <div>@inproceedings{li2020neural,</br>title={Neural architecture search for lightweight
                                non-local networks},</br>author={Li, Yingwei and Jin, Xiaojie and Mei, Jieru and Lian,
                                Xiaochen and Yang, Linjie and Xie, Cihang and Yu, Qihang and Zhou, Yuyin and Bai, Song
                                and Yuille, Alan L},</br>booktitle={Proceedings of the IEEE/CVF Conference on Computer
                                Vision and Pattern Recognition},</br>pages={10297--10306},</br>year={2020}</br>}</div>
                            </td>
                          </tr>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
                <div>
                  <div>
                    <div class="paper show-border">
                      <div>
                        <div class="paper-image"><img src="mei2020atomnas.png" alt></div>
                      </div>
                      <div class="paper-content">
                        <p><b>AtomNAS: Fine-Grained End-to-End Neural Architecture Search</b></p>
                        <p>Jieru Mei, <b>Yingwei Li</b>, Xiaochen Lian, Xiaojie Jin, Linjie Yang, Alan Yuille, Jianchao
                          Yang</p> <i>
                          <p>ICLR, 2020</p>
                        </i> <!----> <span>
                          [<a href="https://arxiv.org/pdf/1912.09640.pdf" target="_blank">Paper</a>]
                        </span> <span>
                          [<a href="https://github.com/meijieru/AtomNAS" target="_blank">Code</a>]
                        </span> <!----> <!----> <!----> <!----> <span>
                          [<a shape="rect" class="toggle">Bibtex</a>]
                        </span>
                        <table bgcolor="#ace5f2" id="abs_AtomNAS: Fine-Grained End-to-End Neural Architecture Search"
                          style="display: none">
                          <tr>
                            <td><span></span></td>
                          </tr>
                        </table>
                        <table bgcolor="#ace5f2" id="AtomNAS: Fine-Grained End-to-End Neural Architecture Search"
                          style="display: none">
                          <tr>
                            <td>
                              <div>@article{mei2019atomnas,</br>title={Atomnas: Fine-grained end-to-end neural
                                architecture search},</br>author={Mei, Jieru and Li, Yingwei and Lian, Xiaochen and Jin,
                                Xiaojie and Yang, Linjie and Yuille, Alan and Yang, Jianchao},</br>journal={arXiv
                                preprint arXiv:1912.09640},</br>year={2019}</br>}</div>
                            </td>
                          </tr>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
                <div>
                  <div>
                    <div class="paper show-border">
                      <div>
                        <div class="paper-image"><img src="li2020learning.jpg" alt></div>
                      </div>
                      <div class="paper-content">
                        <p><b>Learning Transferable Adversarial Examples via Ghost Networks</b></p>
                        <p><b>Yingwei Li</b>, Song Bai, Yuyin Zhou, Cihang Xie, Zhishuai Zhang, Alan Yuille</p> <i>
                          <p>AAAI, 2020</br>CVPR Workshop <b>(Oral)</b>, 2019</p>
                        </i> <!----> <span>
                          [<a href="https://arxiv.org/pdf/1812.03413.pdf" target="_blank">Paper</a>]
                        </span> <span>
                          [<a href="https://github.com/LiYingwei/ghost-network" target="_blank">Code</a>]
                        </span> <!----> <!----> <!----> <!----> <span>
                          [<a shape="rect" class="toggle">Bibtex</a>]
                        </span>
                        <table bgcolor="#ace5f2" id="abs_Learning Transferable Adversarial Examples via Ghost Networks"
                          style="display: none">
                          <tr>
                            <td><span></span></td>
                          </tr>
                        </table>
                        <table bgcolor="#ace5f2" id="Learning Transferable Adversarial Examples via Ghost Networks"
                          style="display: none">
                          <tr>
                            <td>
                              <div>@inproceedings{li2020learning,</br>title={Learning Transferable Adversarial Examples
                                via Ghost Networks},</br>author={Li, Yingwei and Bai, Song and Zhou, Yuyin and Xie,
                                Cihang and Zhang, Zhishuai and Yuille, Alan},</br>booktitle={Proceedings of the AAAI
                                Conference on Artificial Intelligence},</br>volume={34},</br>year={2020}</br>}</div>
                            </td>
                          </tr>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
                <div><!----></div>
                <div><!----></div>
                <div><!----></div>
              </div>
              <div>
                <h1>
                  <font size="6">
                    <p>2019</p>
                  </font>
                </h1>
                <div><!----></div>
                <div><!----></div>
                <div><!----></div>
                <div><!----></div>
                <div><!----></div>
                <div><!----></div>
                <div><!----></div>
                <div><!----></div>
                <div><!----></div>
                <div><!----></div>
                <div><!----></div>
                <div><!----></div>
                <div><!----></div>
                <div><!----></div>
                <div><!----></div>
                <div><!----></div>
                <div><!----></div>
                <div><!----></div>
                <div>
                  <div>
                    <div class="paper show-border">
                      <div>
                        <div class="paper-image"><img src="li2019volumetric.jpg" alt></div>
                      </div>
                      <div class="paper-content">
                        <p><b>Volumetric Medical Image Segmentation: A 3D Deep Coarse-to-Fine Framework and Its
                            Adversarial Examples</b></p>
                        <p><b>Yingwei Li</b>*, Zhuotun Zhu* Yuyin Zhou, Yingda Xia, Wei Shen, Elliot K Fishman, Alan
                          Yuille</p> <i>
                          <p>Book Chapter: Deep Learning and CNN for Medical Image Computing , 2019</p>
                        </i> <!----> <span>
                          [<a href="https://arxiv.org/pdf/2010.16074.pdf" target="_blank">Paper</a>]
                        </span> <!----> <!----> <!----> <!----> <!----> <span>
                          [<a shape="rect" class="toggle">Bibtex</a>]
                        </span>
                        <table bgcolor="#ace5f2"
                          id="abs_Volumetric Medical Image Segmentation: A 3D Deep Coarse-to-Fine Framework and Its Adversarial Examples"
                          style="display: none">
                          <tr>
                            <td><span></span></td>
                          </tr>
                        </table>
                        <table bgcolor="#ace5f2"
                          id="Volumetric Medical Image Segmentation: A 3D Deep Coarse-to-Fine Framework and Its Adversarial Examples"
                          style="display: none">
                          <tr>
                            <td>
                              <div>@Inbook{Li2019,</br>author='Li, Yingwei and Zhu, Zhuotun and Zhou, Yuyin and Xia,
                                Yingda and Shen, Wei and Fishman, Elliot K. and Yuille, Alan L.',</br>title='Volumetric
                                Medical Image Segmentation: A 3D Deep Coarse-to-Fine Framework and Its Adversarial
                                Examples',</br>bookTitle='Deep Learning and Convolutional Neural Networks for Medical
                                Imaging and Clinical Informatics',</br>year='2019',</br>publisher='Springer
                                International
                                Publishing',</br>address='Cham',</br>pages='69--91',</br>isbn='978-3-030-13969-8',</br>doi='10.1007/978-3-030-13969-8_4',</br>url='https://doi.org/10.1007/978-3-030-13969-8_4'</br>}
                              </div>
                            </td>
                          </tr>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
                <div>
                  <div>
                    <div class="paper show-border">
                      <div>
                        <div class="paper-image"><img src="zhou2019hyper.png" alt></div>
                      </div>
                      <div class="paper-content">
                        <p><b>Hyper-Pairing Network for Multi-phase Pancreatic Ductal Adenocarcinoma Segmentation</b>
                        </p>
                        <p>Yuyin Zhou, <b>Yingwei Li</b>, Zhishuai Zhang, Yan Wang, Angtian Wang, Elliot K Fishman, Alan
                          Yuille, Seyoun Park</p> <i>
                          <p>MICCAI, 2019</p>
                        </i> <!----> <span>
                          [<a href="https://arxiv.org/pdf/1909.00906.pdf" target="_blank">Paper</a>]
                        </span> <!----> <!----> <!----> <!----> <!----> <span>
                          [<a shape="rect" class="toggle">Bibtex</a>]
                        </span>
                        <table bgcolor="#ace5f2"
                          id="abs_Hyper-Pairing Network for Multi-phase Pancreatic Ductal Adenocarcinoma Segmentation"
                          style="display: none">
                          <tr>
                            <td><span></span></td>
                          </tr>
                        </table>
                        <table bgcolor="#ace5f2"
                          id="Hyper-Pairing Network for Multi-phase Pancreatic Ductal Adenocarcinoma Segmentation"
                          style="display: none">
                          <tr>
                            <td>
                              <div>@inproceedings{zhou2019hyper,</br>title={Hyper-Pairing Network for Multi-phase
                                Pancreatic Ductal Adenocarcinoma Segmentation},</br>author={Zhou, Yuyin and Li, Yingwei
                                and Zhang, Zhishuai and Wang, Yan and Wang, Angtian and Fishman, Elliot K and Yuille,
                                Alan L and Park, Seyoun},</br>booktitle={International Conference on Medical Image
                                Computing and Computer-Assisted
                                Intervention},</br>pages={155--163},</br>year={2019},</br>organization={Springer}</br>}
                              </div>
                            </td>
                          </tr>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
                <div>
                  <div>
                    <div class="paper show-border">
                      <div>
                        <div class="paper-image"><img src="zhou2019multi.png" alt></div>
                      </div>
                      <div class="paper-content">
                        <p><b>Multi-Scale Attentional Network for Multi-Focal Segmentation of Active Bleed after Pelvic
                            Fractures</b></p>
                        <p>Yuyin Zhou, David Dreizin, <b>Yingwei Li</b>, Zhishuai Zhang, Yan Wang, Alan Yuille</p> <i>
                          <p>MICCAI-MLMI, 2019</p>
                        </i> <!----> <span>
                          [<a href="https://arxiv.org/pdf/1906.09540.pdf" target="_blank">Paper</a>]
                        </span> <!----> <!----> <!----> <!----> <!----> <span>
                          [<a shape="rect" class="toggle">Bibtex</a>]
                        </span>
                        <table bgcolor="#ace5f2"
                          id="abs_Multi-Scale Attentional Network for Multi-Focal Segmentation of Active Bleed after Pelvic Fractures"
                          style="display: none">
                          <tr>
                            <td><span></span></td>
                          </tr>
                        </table>
                        <table bgcolor="#ace5f2"
                          id="Multi-Scale Attentional Network for Multi-Focal Segmentation of Active Bleed after Pelvic Fractures"
                          style="display: none">
                          <tr>
                            <td>
                              <div>@InProceedings{10.1007/978-3-030-32692-0_53,</br>author='Zhou, Yuyin and Dreizin,
                                David and Li, Yingwei and Zhang, Zhishuai and Wang, Yan and Yuille,
                                Alan',</br>title='Multi-scale Attentional Network for Multi-focal Segmentation of Active
                                Bleed After Pelvic Fractures',</br>booktitle='Machine Learning in Medical
                                Imaging',</br>year='2019',</br>publisher='Springer International
                                Publishing',</br>address='Cham',</br>pages='461--469',</br>isbn='978-3-030-32692-0'</br>}
                              </div>
                            </td>
                          </tr>
                        </table>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
        <footer class="page-edit"><!---->
          <div class="last-updated"><span class="prefix">Last Updated:</span> <span class="time">11/13/2022, 3:41:22
              PM</span></div>
        </footer> <!---->
      </main>
      <div class="footer">


</body>

<!-- Mirrored from yingwei.li/publications/ by HTTrack Website Copier/3.x [XR&CO'2014], Wed, 26 Apr 2023 19:32:11 GMT -->

</html>