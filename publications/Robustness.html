<!DOCTYPE html>
<html lang="en-US">
  
<!-- Mirrored from yingwei.li/publications/Robustness by HTTrack Website Copier/3.x [XR&CO'2014], Wed, 26 Apr 2023 19:32:12 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=utf-8" /><!-- /Added by HTTrack -->
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Robust Representation Learning | Yingwei Li</title>
    <meta name="description" content="Yingwei Li. An incoming research scientist @ Waymo.">
    <link rel="icon" href="../logo.jpg">
    
    <link rel="preload" href="../assets/css/0.styles.df94df16.css" as="style"><link rel="preload" href="../assets/js/app.818c8079.js" as="script"><link rel="preload" href="../assets/js/4.0e114e2d.js" as="script"><link rel="preload" href="../assets/js/22.fa778b43.js" as="script"><link rel="preload" href="../assets/js/14.f768ff21.js" as="script"><link rel="preload" href="../assets/js/6.025f0d73.js" as="script"><link rel="prefetch" href="../assets/js/10.ff4d8706.js"><link rel="prefetch" href="../assets/js/11.b7d58d23.js"><link rel="prefetch" href="../assets/js/12.3117e98e.js"><link rel="prefetch" href="../assets/js/13.3fe2f237.js"><link rel="prefetch" href="../assets/js/15.cb229c59.js"><link rel="prefetch" href="../assets/js/16.ea53599e.js"><link rel="prefetch" href="../assets/js/17.4353087f.js"><link rel="prefetch" href="../assets/js/18.b209fa1c.js"><link rel="prefetch" href="../assets/js/19.353daa00.js"><link rel="prefetch" href="../assets/js/2.8b2c6820.js"><link rel="prefetch" href="../assets/js/20.8f421ef9.js"><link rel="prefetch" href="../assets/js/21.a7eb927c.js"><link rel="prefetch" href="../assets/js/23.60c399d5.js"><link rel="prefetch" href="../assets/js/3.46e280fb.js"><link rel="prefetch" href="../assets/js/5.b204be94.js"><link rel="prefetch" href="../assets/js/7.35413e6c.js"><link rel="prefetch" href="../assets/js/8.fde5ed7a.js"><link rel="prefetch" href="../assets/js/9.18b34838.js">
    <link rel="stylesheet" href="../assets/css/0.styles.df94df16.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container no-sidebar"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="../index.html" class="home-link router-link-active"><!----> <span class="site-name">Yingwei Li</span></a> <div class="links"><!----> <nav class="nav-links can-hide"><div class="nav-item"><a href="../index.html" class="nav-link">
  Home
</a></div><div class="nav-item"><a href="index.html" class="nav-link router-link-active">
  Publications
</a></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="../index.html" class="nav-link">
  Home
</a></div><div class="nav-item"><a href="index.html" class="nav-link router-link-active">
  Publications
</a></div> <!----></nav>  <!----> </aside> <main class="page"> <div class="theme-default-content content__default"><div class="tagpapers"><h2>Robust Representation Learning</h2> <div class="tags"><span class="tagpaper"><span><a href="index.html" target="_self">All by Year</a></span></span><span class="tagpaper"><span><a href="3Dperception.html" target="_self">3D Perception</a></span></span><span class="tagpaper"><span><a href="Robustness.html" target="_self"><u>Robustness</u></a></span></span><span class="tagpaper"><span><a href="MultiModality.html" target="_self">Multi Modality</a></span></span><span class="tagpaper"><span><a href="AutoML.html" target="_self">AutoML</a></span></span></div> <br> <div class="note"><font size="3" color="rgb(0,0,0)" style="font-weight:normal;">Representative papers are shown with red border</font></div></div> <div class="papers"><div><div><div><!----></div></div><div><div><!----></div></div><div><div><div><div class="paper show-border"><div><div class="paper-image"><img src="liu2022harnessing.jpg" alt></div></div> <div class="paper-content"><p><b>Harnessing Perceptual Adversarial Patches for Crowd Counting</b></p> <p>Shunchang Liu*, Jiakai Wang*, Aishan Liu, <b>Yingwei Li</b>, Yijie Gao, Xianglong Liu, Dacheng Tao</p> <i><p>ACM CCS, 2022</p></i> <!----> <span>
                [<a href="https://dl.acm.org/doi/pdf/10.1145/3548606.3560566" target="_blank">Paper</a>]
              </span> <span>
                [<a href="https://github.com/shunchang-liu/PAP-Pytorch" target="_blank">Code</a>]
              </span> <!----> <!----> <!----> <!----> <span>
                [<a shape="rect" class="toggle">Bibtex</a>]
              </span> <table bgcolor="#ace5f2" id="abs_Harnessing Perceptual Adversarial Patches for Crowd Counting" style="display: none"><tr><td><span></span></td></tr></table> <table bgcolor="#ace5f2" id="Harnessing Perceptual Adversarial Patches for Crowd Counting" style="display: none"><tr><td><div>@inproceedings{liu2022harnessing,</br>  title={Harnessing Perceptual Adversarial Patches for Crowd Counting},</br>  author={Liu, Shunchang and Wang, Jiakai and Liu, Aishan and Li, Yingwei and Gao, Yijie and Liu, Xianglong and Tao, Dacheng},</br>  booktitle={Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security},</br>  pages={2055--2069},</br>  year={2022}</br>}</div></td></tr></table></div></div></div></div></div><div><div><!----></div><div><div><div class="paper show-border"><div><div class="paper-image"><img src="guo2022context.png" alt></div></div> <div class="paper-content"><p><b>Context Enhanced Stereo Transformer</b></p> <p>Weiyu Guo, Zhaoshuo Li, Yongkui Yang, Zheng Wang, Russ Taylor, Mathias Unberath, Alan Yuille, <b>Yingwei Li</b></p> <i><p>ECCV, 2022</p></i> <!----> <span>
                [<a href="https://arxiv.org/pdf/2210.11719.pdf" target="_blank">Paper</a>]
              </span> <span>
                [<a href="https://github.com/guoweiyu/Context-Enhanced-Stereo-Transformer" target="_blank">Code</a>]
              </span> <!----> <!----> <!----> <!----> <span>
                [<a shape="rect" class="toggle">Bibtex</a>]
              </span> <table bgcolor="#ace5f2" id="abs_Context Enhanced Stereo Transformer" style="display: none"><tr><td><span></span></td></tr></table> <table bgcolor="#ace5f2" id="Context Enhanced Stereo Transformer" style="display: none"><tr><td><div>@inproceedings{guo2022context,</br>title={Context Enhanced Stereo Transformer},</br>author={Guo, Weiyu and Li, Zhaoshuo and Yang, Yongkui and Wang, Zheng and Taylor, Russ and Unberath, Mathias and Yuille, Alan and Li, Yingwei},</br>booktitle={ECCV},</br>year={2022}</br>}</div></td></tr></table></div></div></div></div></div><div><div><!----></div><div><!----></div><div><div><div class="paper show-border highlight"><div><div class="paper-image"><img src="li2022lidarcamera.jpg" alt></div></div> <div class="paper-content"><p><b>DeepFusion: Lidar-Camera Deep Fusion for Multi-Modal 3D Object Detection</b></p> <p><b>Yingwei Li</b>, Adams Yu, Tianjian Meng, Ben Caine, Jiquan Ngiam, Daiyi Peng, Junyang Shen, Bo Wu, Yifeng Lu, Denny Zhou, Quoc Le, Alan Yuille, Mingxing Tan</p> <i><p>CVPR, 2022</p></i> <!----> <span>
                [<a href="https://arxiv.org/pdf/2203.08195.pdf" target="_blank">Paper</a>]
              </span> <span>
                [<a href="https://github.com/tensorflow/lingvo" target="_blank">Code</a>]
              </span> <span>
                [<a href="https://ai.googleblog.com/2022/04/lidar-camera-deep-fusion-for-multi.html" target="_blank">Website</a>]
              </span> <!----> <!----> <!----> <span>
                [<a shape="rect" class="toggle">Bibtex</a>]
              </span> <table bgcolor="#ace5f2" id="abs_DeepFusion: Lidar-Camera Deep Fusion for Multi-Modal 3D Object Detection" style="display: none"><tr><td><span></span></td></tr></table> <table bgcolor="#ace5f2" id="DeepFusion: Lidar-Camera Deep Fusion for Multi-Modal 3D Object Detection" style="display: none"><tr><td><div>@article{li2022deepfusion,</br>title={DeepFusion: Lidar-Camera Deep Fusion for Multi-Modal 3D Object Detection},</br>author={Li, Yingwei and Yu, Adams Wei and Meng, Tianjian and Caine, Ben and Ngiam, Jiquan and Peng, Daiyi and Shen, Junyang and Wu, Bo and Lu, Yifeng and Zhou, Denny and others},</br>journal={arXiv preprint arXiv:2203.08195},</br>year={2022}</br>}</div></td></tr></table></div></div></div></div></div><div><div><!----></div></div><div><div><div><div class="paper show-border"><div><div class="paper-image"><img src="gupta2022swapmix.png" alt></div></div> <div class="paper-content"><p><b>SwapMix: Diagnosing and Regularizing the Over-reliance on Visual Context in Visual Question Answering</b></p> <p>Vipul Gupta, Zhuowan Li, Adam Kortylewski, Chenyu Zhang, <b>Yingwei Li</b>, Alan Yuille</p> <i><p>CVPR, 2022</p></i> <!----> <span>
                [<a href="https://arxiv.org/pdf/2204.02285.pdf" target="_blank">Paper</a>]
              </span> <!----> <!----> <!----> <!----> <!----> <span>
                [<a shape="rect" class="toggle">Bibtex</a>]
              </span> <table bgcolor="#ace5f2" id="abs_SwapMix: Diagnosing and Regularizing the Over-reliance on Visual Context in Visual Question Answering" style="display: none"><tr><td><span></span></td></tr></table> <table bgcolor="#ace5f2" id="SwapMix: Diagnosing and Regularizing the Over-reliance on Visual Context in Visual Question Answering" style="display: none"><tr><td><div>@article{gupta2022swapmix,</br>title={SwapMix: Diagnosing and Regularizing the Over-Reliance on Visual Context in Visual Question Answering},</br>author={Gupta, Vipul and Li, Zhuowan and Kortylewski, Adam and Zhang, Chenyu and Li, Yingwei and Yuille, Alan},</br>journal={arXiv preprint arXiv:2204.02285},</br>year={2022}</br>}</div></td></tr></table></div></div></div></div></div><div><div><!----></div></div><div><div><!----></div><div><div><div class="paper show-border highlight"><div><div class="paper-image"><img src="li2022r4d.jpg" alt></div></div> <div class="paper-content"><p><b>R4D: Utilizing Reference Objects for Long-Range Distance Estimation</b></p> <p><b>Yingwei Li</b>, Tiffany Chen, Maya Kabkab, Ruichi Yu, Longlong Jing, Yurong You, Hang Zhao</p> <i><p>ICLR, 2022</p></i> <!----> <span>
                [<a href="https://openreview.net/pdf?id=MQ2sAGunyBP" target="_blank">Paper</a>]
              </span> <!----> <!----> <!----> <!----> <span>
                [<a href="../data/iclr22-r4d-supp.pdf" target="_blank">Supplementary</a>]
              </span> <span>
                [<a shape="rect" class="toggle">Bibtex</a>]
              </span> <table bgcolor="#ace5f2" id="abs_R4D: Utilizing Reference Objects for Long-Range Distance Estimation" style="display: none"><tr><td><span></span></td></tr></table> <table bgcolor="#ace5f2" id="R4D: Utilizing Reference Objects for Long-Range Distance Estimation" style="display: none"><tr><td><div>@inproceedings{li2021r4d,</br>title={R4D: Utilizing Reference Objects for Long-Range Distance Estimation},</br>author={Li, Yingwei and Chen, Tiffany and Kabkab, Maya and Yu, Ruichi and Jing, Longlong and You, Yurong and Zhao, Hang},</br>booktitle={International Conference on Learning Representations},</br>year={2021}</br>}</div></td></tr></table></div></div></div></div></div><div><div><div><div class="paper show-border"><div><div class="paper-image"><img src="mei2022fast.jpg" alt></div></div> <div class="paper-content"><p><b>Fast AdvProp</b></p> <p>Jieru Mei, Yucheng Han, Yutong Bai, Yixiao Zhang, <b>Yingwei Li</b>, Xianhang Li, Alan Yuille, Cihang Xie</p> <i><p>ICLR, 2022</p></i> <!----> <span>
                [<a href="https://openreview.net/pdf?id=hcoswsDHNAW" target="_blank">Paper</a>]
              </span> <!----> <!----> <!----> <!----> <!----> <span>
                [<a shape="rect" class="toggle">Bibtex</a>]
              </span> <table bgcolor="#ace5f2" id="abs_Fast AdvProp" style="display: none"><tr><td><span></span></td></tr></table> <table bgcolor="#ace5f2" id="Fast AdvProp" style="display: none"><tr><td><div>@inproceedings{mei2021fast,</br>title={Fast AdvProp},</br>author={Mei, Jieru and Han, Yucheng and Bai, Yutong and Zhang, Yixiao and Li, Yingwei and Li, Xianhang and Yuille, Alan and Xie, Cihang},</br>booktitle={International Conference on Learning Representations},</br>year={2021}</br>}</div></td></tr></table></div></div></div></div></div><div><div><!----></div></div><div><div><div><div class="paper show-border highlight"><div><div class="paper-image"><img src="li2020shapetexture.jpg" alt></div></div> <div class="paper-content"><p><b>Shape-Texture Debiased Neural Network Training</b></p> <p><b>Yingwei Li</b>, Qihang Yu, Mingxing Tan, Jieru Mei, Peng Tang, Wei Shen, Alan Yuille, Cihang Xie</p> <i><p>ICLR, 2021</p></i> <!----> <span>
                [<a href="https://arxiv.org/pdf/2010.05981.pdf" target="_blank">Paper</a>]
              </span> <span>
                [<a href="https://github.com/LiYingwei/ShapeTextureDebiasedTraining" target="_blank">Code</a>]
              </span> <span>
                [<a href="https://iclr.cc/media/iclr-2021/Slides/2870.pdf" target="_blank">Website</a>]
              </span> <span>
                [<a href="https://slideslive.com/38953458/shapetexture-debiased-neural-network-training" target="_blank">Video</a>]
              </span> <!----> <!----> <span>
                [<a shape="rect" class="toggle">Bibtex</a>]
              </span> <table bgcolor="#ace5f2" id="abs_Shape-Texture Debiased Neural Network Training" style="display: none"><tr><td><span></span></td></tr></table> <table bgcolor="#ace5f2" id="Shape-Texture Debiased Neural Network Training" style="display: none"><tr><td><div>@article{li2020shape,</br>title={Shape-texture debiased neural network training},</br>author={Li, Yingwei and Yu, Qihang and Tan, Mingxing and Mei, Jieru and Tang, Peng and Shen, Wei and Yuille, Alan and Xie, Cihang},</br>journal={arXiv preprint arXiv:2010.05981},</br>year={2020}</br>}</div></td></tr></table></div></div></div></div></div><div><div><!----></div></div><div><div><div><div class="paper show-border"><div><div class="paper-image"><img src="bai2019adversarial.png" alt></div></div> <div class="paper-content"><p><b>Adversarial Metric Attack and Defense for Person Re-identification</b></p> <p>Song Bai, <b>Yingwei Li</b>, Yuyin Zhou, Qizhu Li, Philip H.S. Torr</p> <i><p>TPAMI, 2020</p></i> <!----> <span>
                [<a href="https://arxiv.org/pdf/1901.10650.pdf" target="_blank">Paper</a>]
              </span> <span>
                [<a href="https://github.com/SongBaiHust/Adversarial_Metric_Attack" target="_blank">Code</a>]
              </span> <!----> <!----> <!----> <!----> <span>
                [<a shape="rect" class="toggle">Bibtex</a>]
              </span> <table bgcolor="#ace5f2" id="abs_Adversarial Metric Attack and Defense for Person Re-identification" style="display: none"><tr><td><span></span></td></tr></table> <table bgcolor="#ace5f2" id="Adversarial Metric Attack and Defense for Person Re-identification" style="display: none"><tr><td><div>@article{bai2020adversarial,</br>title={Adversarial metric attack and defense for person re-identification},</br>author={Bai, Song and Li, Yingwei and Zhou, Yuyin and Li, Qizhu and Torr, Philip HS},</br>journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},</br>volume={43},</br>number={6},</br>pages={2119--2126},</br>year={2020},</br>publisher={IEEE}</br>}</div></td></tr></table></div></div></div></div></div><div><div><div><div class="paper show-border"><div><div class="paper-image"><img src="li2019regional.jpg" alt></div></div> <div class="paper-content"><p><b>Regional Homogeneity: Towards Learning Transferable Universal Adversarial Perturbations Against Defenses</b></p> <p><b>Yingwei Li</b>, Song Bai, Cihang Xie, Zhenyu Liao, Xiaohui Shen, Alan Yuille</p> <i><p>ECCV, 2020</p></i> <!----> <span>
                [<a href="https://arxiv.org/pdf/1904.00979.pdf" target="_blank">Paper</a>]
              </span> <span>
                [<a href="https://github.com/LiYingwei/Regional-Homogeneity" target="_blank">Code</a>]
              </span> <!----> <!----> <!----> <!----> <span>
                [<a shape="rect" class="toggle">Bibtex</a>]
              </span> <table bgcolor="#ace5f2" id="abs_Regional Homogeneity: Towards Learning Transferable Universal Adversarial Perturbations Against Defenses" style="display: none"><tr><td><span></span></td></tr></table> <table bgcolor="#ace5f2" id="Regional Homogeneity: Towards Learning Transferable Universal Adversarial Perturbations Against Defenses" style="display: none"><tr><td><div>@inproceedings{li2020regional,</br>title={Regional homogeneity: Towards learning transferable universal adversarial perturbations against defenses},</br>author={Li, Yingwei and Bai, Song and Xie, Cihang and Liao, Zhenyu and Shen, Xiaohui and Yuille, Alan},</br>booktitle={European Conference on Computer Vision},</br>pages={795--813},</br>year={2020},</br>organization={Springer}</br>}</div></td></tr></table></div></div></div></div></div><div><div><!----></div></div><div><div><!----></div></div><div><div><div><div class="paper show-border"><div><div class="paper-image"><img src="li2020learning.jpg" alt></div></div> <div class="paper-content"><p><b>Learning Transferable Adversarial Examples via Ghost Networks</b></p> <p><b>Yingwei Li</b>, Song Bai, Yuyin Zhou, Cihang Xie, Zhishuai Zhang, Alan Yuille</p> <i><p>AAAI, 2020</br>CVPR Workshop <b>(Oral)</b>, 2019</p></i> <!----> <span>
                [<a href="https://arxiv.org/pdf/1812.03413.pdf" target="_blank">Paper</a>]
              </span> <span>
                [<a href="https://github.com/LiYingwei/ghost-network" target="_blank">Code</a>]
              </span> <!----> <!----> <!----> <!----> <span>
                [<a shape="rect" class="toggle">Bibtex</a>]
              </span> <table bgcolor="#ace5f2" id="abs_Learning Transferable Adversarial Examples via Ghost Networks" style="display: none"><tr><td><span></span></td></tr></table> <table bgcolor="#ace5f2" id="Learning Transferable Adversarial Examples via Ghost Networks" style="display: none"><tr><td><div>@inproceedings{li2020learning,</br>title={Learning Transferable Adversarial Examples via Ghost Networks},</br>author={Li, Yingwei and Bai, Song and Zhou, Yuyin and Xie, Cihang and Zhang, Zhishuai and Yuille, Alan},</br>booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},</br>volume={34},</br>year={2020}</br>}</div></td></tr></table></div></div></div></div></div><div><div><div><div class="paper show-border"><div><div class="paper-image"><img src="li2019volumetric.jpg" alt></div></div> <div class="paper-content"><p><b>Volumetric Medical Image Segmentation: A 3D Deep Coarse-to-Fine Framework and Its Adversarial Examples</b></p> <p><b>Yingwei Li</b>*, Zhuotun Zhu* Yuyin Zhou, Yingda Xia, Wei Shen, Elliot K Fishman, Alan Yuille</p> <i><p>Book Chapter: Deep Learning and CNN for Medical Image Computing , 2019</p></i> <!----> <span>
                [<a href="https://arxiv.org/pdf/2010.16074.pdf" target="_blank">Paper</a>]
              </span> <!----> <!----> <!----> <!----> <!----> <span>
                [<a shape="rect" class="toggle">Bibtex</a>]
              </span> <table bgcolor="#ace5f2" id="abs_Volumetric Medical Image Segmentation: A 3D Deep Coarse-to-Fine Framework and Its Adversarial Examples" style="display: none"><tr><td><span></span></td></tr></table> <table bgcolor="#ace5f2" id="Volumetric Medical Image Segmentation: A 3D Deep Coarse-to-Fine Framework and Its Adversarial Examples" style="display: none"><tr><td><div>@Inbook{Li2019,</br>author='Li, Yingwei and Zhu, Zhuotun and Zhou, Yuyin and Xia, Yingda and Shen, Wei and Fishman, Elliot K. and Yuille, Alan L.',</br>title='Volumetric Medical Image Segmentation: A 3D Deep Coarse-to-Fine Framework and Its Adversarial Examples',</br>bookTitle='Deep Learning and Convolutional Neural Networks for Medical Imaging and Clinical Informatics',</br>year='2019',</br>publisher='Springer International Publishing',</br>address='Cham',</br>pages='69--91',</br>isbn='978-3-030-13969-8',</br>doi='10.1007/978-3-030-13969-8_4',</br>url='https://doi.org/10.1007/978-3-030-13969-8_4'</br>}</div></td></tr></table></div></div></div></div></div><div><div><!----></div></div><div></div></div></div></div> <footer class="page-edit"><!----> <div class="last-updated"><span class="prefix">Last Updated:</span> <span class="time">11/13/2022, 3:41:22 PM</span></div></footer> <!----> </main> <div class="footer">
    
    Created by my loving girlfriend
    <a href="https://avaxiao.github.io/" target="_blank" title="Yao Xiao"><span style="color:#4693B8;">Yao Xiao</span></a> <br><br><script type="text/javascript" id="clustrmaps" src="../../clustrmaps.com/map_v2ec66.js?cl=ffffff&amp;w=275&amp;t=tt&amp;d=YOaHyTs10B4AATx9wnHLidoS1nTXoaUn3QKr3QqvDZk"></script></div></div><div class="global-ui"></div></div>
    <script src="../assets/js/app.818c8079.js" defer></script><script src="../assets/js/4.0e114e2d.js" defer></script><script src="../assets/js/22.fa778b43.js" defer></script><script src="../assets/js/14.f768ff21.js" defer></script><script src="../assets/js/6.025f0d73.js" defer></script>
  </body>

<!-- Mirrored from yingwei.li/publications/Robustness by HTTrack Website Copier/3.x [XR&CO'2014], Wed, 26 Apr 2023 19:32:12 GMT -->
</html>
