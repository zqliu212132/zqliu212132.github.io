<!DOCTYPE html>
<html lang="en-US">
  
<!-- Mirrored from yingwei.li/publications/3Dperception by HTTrack Website Copier/3.x [XR&CO'2014], Wed, 26 Apr 2023 19:32:12 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=utf-8" /><!-- /Added by HTTrack -->
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>3D perception | Yingwei Li</title>
    <meta name="description" content="Yingwei Li. An incoming research scientist @ Waymo.">
    <link rel="icon" href="../logo.jpg">
    
    <link rel="preload" href="../assets/css/0.styles.df94df16.css" as="style"><link rel="preload" href="../assets/js/app.818c8079.js" as="script"><link rel="preload" href="../assets/js/4.0e114e2d.js" as="script"><link rel="preload" href="../assets/js/19.353daa00.js" as="script"><link rel="preload" href="../assets/js/14.f768ff21.js" as="script"><link rel="preload" href="../assets/js/6.025f0d73.js" as="script"><link rel="prefetch" href="../assets/js/10.ff4d8706.js"><link rel="prefetch" href="../assets/js/11.b7d58d23.js"><link rel="prefetch" href="../assets/js/12.3117e98e.js"><link rel="prefetch" href="../assets/js/13.3fe2f237.js"><link rel="prefetch" href="../assets/js/15.cb229c59.js"><link rel="prefetch" href="../assets/js/16.ea53599e.js"><link rel="prefetch" href="../assets/js/17.4353087f.js"><link rel="prefetch" href="../assets/js/18.b209fa1c.js"><link rel="prefetch" href="../assets/js/2.8b2c6820.js"><link rel="prefetch" href="../assets/js/20.8f421ef9.js"><link rel="prefetch" href="../assets/js/21.a7eb927c.js"><link rel="prefetch" href="../assets/js/22.fa778b43.js"><link rel="prefetch" href="../assets/js/23.60c399d5.js"><link rel="prefetch" href="../assets/js/3.46e280fb.js"><link rel="prefetch" href="../assets/js/5.b204be94.js"><link rel="prefetch" href="../assets/js/7.35413e6c.js"><link rel="prefetch" href="../assets/js/8.fde5ed7a.js"><link rel="prefetch" href="../assets/js/9.18b34838.js">
    <link rel="stylesheet" href="../assets/css/0.styles.df94df16.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container no-sidebar"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="../index.html" class="home-link router-link-active"><!----> <span class="site-name">Yingwei Li</span></a> <div class="links"><!----> <nav class="nav-links can-hide"><div class="nav-item"><a href="../index.html" class="nav-link">
  Home
</a></div><div class="nav-item"><a href="index.html" class="nav-link router-link-active">
  Publications
</a></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="../index.html" class="nav-link">
  Home
</a></div><div class="nav-item"><a href="index.html" class="nav-link router-link-active">
  Publications
</a></div> <!----></nav>  <!----> </aside> <main class="page"> <div class="theme-default-content content__default"><div class="tagpapers"><h2>3D perception</h2> <div class="tags"><span class="tagpaper"><span><a href="index.html" target="_self">All by Year</a></span></span><span class="tagpaper"><span><a href="3Dperception.html" target="_self"><u>3D Perception</u></a></span></span><span class="tagpaper"><span><a href="Robustness.html" target="_self">Robustness</a></span></span><span class="tagpaper"><span><a href="MultiModality.html" target="_self">Multi Modality</a></span></span><span class="tagpaper"><span><a href="AutoML.html" target="_self">AutoML</a></span></span></div> <br> <div class="note"><font size="3" color="rgb(0,0,0)" style="font-weight:normal;">Representative papers are shown with red border</font></div></div> <div class="papers"><div><div><div><!----></div></div><div><div><div><div class="paper show-border"><div><div class="paper-image"><img src="chen2023class.jpg" alt></div></div> <div class="paper-content"><p><b>Class-Level Confidence Based 3D Semi-Supervised Learning</b></p> <p>Zhimin Chen, Longlong Jing, Liang Yang, <b>Yingwei Li</b>, and Bing Li</p> <i><p>WACV, 2023</p></i> <!----> <span>
                [<a href="https://arxiv.org/pdf/2210.10138.pdf" target="_blank">Paper</a>]
              </span> <!----> <!----> <!----> <!----> <!----> <span>
                [<a shape="rect" class="toggle">Bibtex</a>]
              </span> <table bgcolor="#ace5f2" id="abs_Class-Level Confidence Based 3D Semi-Supervised Learning" style="display: none"><tr><td><span></span></td></tr></table> <table bgcolor="#ace5f2" id="Class-Level Confidence Based 3D Semi-Supervised Learning" style="display: none"><tr><td><div>@article{chen2022class,</br>  title={Class-Level Confidence Based 3D Semi-Supervised Learning},</br>  author={Chen, Zhimin and Jing, Longlong and Yang, Liang and Li, Bing},</br>  journal={arXiv preprint arXiv:2210.10138},</br>  year={2022}</br>}</div></td></tr></table></div></div></div></div></div><div><div><!----></div></div><div><div><div><div class="paper show-border"><div><div class="paper-image"><img src="guo2022context.png" alt></div></div> <div class="paper-content"><p><b>Context Enhanced Stereo Transformer</b></p> <p>Weiyu Guo, Zhaoshuo Li, Yongkui Yang, Zheng Wang, Russ Taylor, Mathias Unberath, Alan Yuille, <b>Yingwei Li</b></p> <i><p>ECCV, 2022</p></i> <!----> <span>
                [<a href="https://arxiv.org/pdf/2210.11719.pdf" target="_blank">Paper</a>]
              </span> <span>
                [<a href="https://github.com/guoweiyu/Context-Enhanced-Stereo-Transformer" target="_blank">Code</a>]
              </span> <!----> <!----> <!----> <!----> <span>
                [<a shape="rect" class="toggle">Bibtex</a>]
              </span> <table bgcolor="#ace5f2" id="abs_Context Enhanced Stereo Transformer" style="display: none"><tr><td><span></span></td></tr></table> <table bgcolor="#ace5f2" id="Context Enhanced Stereo Transformer" style="display: none"><tr><td><div>@inproceedings{guo2022context,</br>title={Context Enhanced Stereo Transformer},</br>author={Guo, Weiyu and Li, Zhaoshuo and Yang, Yongkui and Wang, Zheng and Taylor, Russ and Unberath, Mathias and Yuille, Alan and Li, Yingwei},</br>booktitle={ECCV},</br>year={2022}</br>}</div></td></tr></table></div></div></div></div><div><!----></div></div><div><div><div><div class="paper show-border highlight"><div><div class="paper-image"><img src="li2022lidarcamera.jpg" alt></div></div> <div class="paper-content"><p><b>DeepFusion: Lidar-Camera Deep Fusion for Multi-Modal 3D Object Detection</b></p> <p><b>Yingwei Li</b>, Adams Yu, Tianjian Meng, Ben Caine, Jiquan Ngiam, Daiyi Peng, Junyang Shen, Bo Wu, Yifeng Lu, Denny Zhou, Quoc Le, Alan Yuille, Mingxing Tan</p> <i><p>CVPR, 2022</p></i> <!----> <span>
                [<a href="https://arxiv.org/pdf/2203.08195.pdf" target="_blank">Paper</a>]
              </span> <span>
                [<a href="https://github.com/tensorflow/lingvo" target="_blank">Code</a>]
              </span> <span>
                [<a href="https://ai.googleblog.com/2022/04/lidar-camera-deep-fusion-for-multi.html" target="_blank">Website</a>]
              </span> <!----> <!----> <!----> <span>
                [<a shape="rect" class="toggle">Bibtex</a>]
              </span> <table bgcolor="#ace5f2" id="abs_DeepFusion: Lidar-Camera Deep Fusion for Multi-Modal 3D Object Detection" style="display: none"><tr><td><span></span></td></tr></table> <table bgcolor="#ace5f2" id="DeepFusion: Lidar-Camera Deep Fusion for Multi-Modal 3D Object Detection" style="display: none"><tr><td><div>@article{li2022deepfusion,</br>title={DeepFusion: Lidar-Camera Deep Fusion for Multi-Modal 3D Object Detection},</br>author={Li, Yingwei and Yu, Adams Wei and Meng, Tianjian and Caine, Ben and Ngiam, Jiquan and Peng, Daiyi and Shen, Junyang and Wu, Bo and Lu, Yifeng and Zhou, Denny and others},</br>journal={arXiv preprint arXiv:2203.08195},</br>year={2022}</br>}</div></td></tr></table></div></div></div></div><div><!----></div><div><!----></div></div><div><div><!----></div></div><div><div><!----></div></div><div><div><div><div class="paper show-border"><div><div class="paper-image"><img src="jing2022depth.jpg" alt></div></div> <div class="paper-content"><p><b>Depth Estimation Matters Most: Improving Per-Object Depth Estimation for Monocular 3D Detection and Tracking</b></p> <p>Longlong Jing, Ruichi Yu, Jiyang Gao, Henrik Kretzschmar, Kang Li, Charles R. Qi, Hang Zhao, Alper Ayvaci, Xu Chen, Dillon Cower, <b>Yingwei Li</b>, Yurong You, Han Deng, Congcong Li, Dragomir Anguelov</p> <i><p>ICRA, 2022</p></i> <!----> <!----> <!----> <!----> <!----> <!----> <!----> <!----> <table bgcolor="#ace5f2" id="abs_Depth Estimation Matters Most: Improving Per-Object Depth Estimation for Monocular 3D Detection and Tracking" style="display: none"><tr><td><span></span></td></tr></table> <table bgcolor="#ace5f2" id="Depth Estimation Matters Most: Improving Per-Object Depth Estimation for Monocular 3D Detection and Tracking" style="display: none"><tr><td><div></div></td></tr></table></div></div></div></div></div><div><div><div><div class="paper show-border highlight"><div><div class="paper-image"><img src="li2022r4d.jpg" alt></div></div> <div class="paper-content"><p><b>R4D: Utilizing Reference Objects for Long-Range Distance Estimation</b></p> <p><b>Yingwei Li</b>, Tiffany Chen, Maya Kabkab, Ruichi Yu, Longlong Jing, Yurong You, Hang Zhao</p> <i><p>ICLR, 2022</p></i> <!----> <span>
                [<a href="https://openreview.net/pdf?id=MQ2sAGunyBP" target="_blank">Paper</a>]
              </span> <!----> <!----> <!----> <!----> <span>
                [<a href="../data/iclr22-r4d-supp.pdf" target="_blank">Supplementary</a>]
              </span> <span>
                [<a shape="rect" class="toggle">Bibtex</a>]
              </span> <table bgcolor="#ace5f2" id="abs_R4D: Utilizing Reference Objects for Long-Range Distance Estimation" style="display: none"><tr><td><span></span></td></tr></table> <table bgcolor="#ace5f2" id="R4D: Utilizing Reference Objects for Long-Range Distance Estimation" style="display: none"><tr><td><div>@inproceedings{li2021r4d,</br>title={R4D: Utilizing Reference Objects for Long-Range Distance Estimation},</br>author={Li, Yingwei and Chen, Tiffany and Kabkab, Maya and Yu, Ruichi and Jing, Longlong and You, Yurong and Zhao, Hang},</br>booktitle={International Conference on Learning Representations},</br>year={2021}</br>}</div></td></tr></table></div></div></div></div><div><!----></div></div><div><div><!----></div></div><div><div><!----></div></div><div><div><!----></div></div><div><div><!----></div></div><div><div><!----></div></div><div><div><!----></div></div><div><div><!----></div></div><div><div><!----></div></div><div><div><!----></div></div><div><div><!----></div></div><div><div><!----></div></div><div></div></div></div></div> <footer class="page-edit"><!----> <div class="last-updated"><span class="prefix">Last Updated:</span> <span class="time">11/13/2022, 3:41:22 PM</span></div></footer> <!----> </main> <div class="footer">
    
    Created by my loving girlfriend
    <a href="https://avaxiao.github.io/" target="_blank" title="Yao Xiao"><span style="color:#4693B8;">Yao Xiao</span></a> <br><br><script type="text/javascript" id="clustrmaps" src="../../clustrmaps.com/map_v2ec66.js?cl=ffffff&amp;w=275&amp;t=tt&amp;d=YOaHyTs10B4AATx9wnHLidoS1nTXoaUn3QKr3QqvDZk"></script></div></div><div class="global-ui"></div></div>
    <script src="../assets/js/app.818c8079.js" defer></script><script src="../assets/js/4.0e114e2d.js" defer></script><script src="../assets/js/19.353daa00.js" defer></script><script src="../assets/js/14.f768ff21.js" defer></script><script src="../assets/js/6.025f0d73.js" defer></script>
  </body>

<!-- Mirrored from yingwei.li/publications/3Dperception by HTTrack Website Copier/3.x [XR&CO'2014], Wed, 26 Apr 2023 19:32:12 GMT -->
</html>
